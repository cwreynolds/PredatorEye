{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5CjJNUz-2Tm"
      },
      "source": [
        "# Parameters and imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TDmnBDIK6wP9"
      },
      "outputs": [],
      "source": [
        "# random_seed = 20211216\n",
        "# random_seed = 20211218\n",
        "# random_seed = 20211221\n",
        "random_seed = 20211222\n",
        "\n",
        "# Use the \"mini\" training set with 9 images.\n",
        "# fcd_ts_dir = '/content/drive/My Drive/PredatorEye/mini_training_set/'\n",
        "# Use the \"real\" training set with 2008 images\n",
        "fcd_ts_dir = '/content/drive/My Drive/PredatorEye/fcd_training_set/'\n",
        "\n",
        "model_save_directory = '/content/drive/My Drive/PredatorEye/saved_models/'\n",
        "\n",
        "# max_input_images = 100\n",
        "max_input_images = 'all'\n",
        "\n",
        "# For each image read from dataset \"amplify\" the set by up to 7 additional\n",
        "# variations of the image via rotations and mirroring.\n",
        "# When I tried using 8 I would run out of memory when the training began.\n",
        "# amplification = 1\n",
        "# amplification = 6\n",
        "amplification = 8\n",
        "\n",
        "# amplification = 16 # add intensity inversion, filled memory\n",
        "# amplification = 12\n",
        "# amplification = 10\n",
        "# amplification = 9\n",
        "\n",
        "# 20211221\n",
        "# Allow amplification by anopther factor of 2 by inverting image brightness.\n",
        "# allow_amp_by_inv = True\n",
        "allow_amp_by_inv = False\n",
        "\n",
        "# Maybe read from image file?\n",
        "# Maybe assert all images are square and this size?\n",
        "fcd_image_size = 1024\n",
        "\n",
        "# Disk diameter, relative to full sized megapixel image.\n",
        "fcd_disk_size = 201\n",
        "\n",
        "# For scaling down the input image size.\n",
        "# input_scale = 1\n",
        "input_scale = 0.125\n",
        "# input_scale = 0.25\n",
        "# input_scale = 0.5\n",
        "if (input_scale != 1):\n",
        "    fcd_image_size = int(fcd_image_size * input_scale)\n",
        "    # does this really want to be an int?\n",
        "    fcd_disk_size = int(fcd_disk_size * input_scale)\n",
        "\n",
        "fcd_epochs = 100\n",
        "# fcd_epochs = 40\n",
        "\n",
        "# On 20211218 increased from 32 to 128 which seemed to help stability.\n",
        "fcd_batch_size = 128\n",
        "# # 20211221\n",
        "# fcd_batch_size = 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-QjNIB8oGDm",
        "outputId": "fbdd57ba-be28-4c7d-e0b0-fd6bf0e2b480"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.7.0\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "print('TensorFlow version:', tf.__version__)\n",
        "\n",
        "import gc\n",
        "import PIL\n",
        "import time\n",
        "import pytz\n",
        "import random\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from os import listdir\n",
        "from os.path import join\n",
        "from tqdm.auto import tqdm\n",
        "from matplotlib import image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# from DLAVA, includes unused symbols, maybe prune later\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.constraints import MaxNorm\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from tensorflow.keras.losses import Loss\n",
        "\n",
        "## maybe just write these inline in the code below?\n",
        "from numpy import asarray \n",
        "from tensorflow.keras import backend as keras_backend\n",
        "keras_backend.set_image_data_format('channels_last')\n",
        "\n",
        "# 20211218\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from datetime import datetime\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDFOLYNF0oSj",
        "outputId": "811fda06-f9ce-447c-97b8-c9c46c21c2b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Dec 22 18:41:47 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "Using a high-RAM runtime.\n",
            "Your runtime has 27.3 gigabytes of available RAM\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Check for Colab Pro resources\n",
        "def check_colab_resources():\n",
        "    gpu_info = !nvidia-smi\n",
        "    gpu_info = '\\n'.join(gpu_info)\n",
        "    if gpu_info.find('failed') >= 0:\n",
        "        print('Not connected to a GPU')\n",
        "    else:\n",
        "        print(gpu_info)\n",
        "    from psutil import virtual_memory\n",
        "    ram_gb = virtual_memory().total / 1e9\n",
        "    if ram_gb < 20:\n",
        "        print('Not using a high-RAM runtime.')\n",
        "    else:\n",
        "        print('Using a high-RAM runtime.')\n",
        "    print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "check_colab_resources()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCQFpDu-B0HU"
      },
      "source": [
        "# Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8EhCRj-wGwpE"
      },
      "outputs": [],
      "source": [
        "# Prints \"expression = <value>\"\n",
        "def debug_print(expression):\n",
        "    print(expression, '=', eval(expression))\n",
        "\n",
        "# Reset random sequence seeds in Python's \"random\", Numpy, and TensorFlow.\n",
        "def reset_random_seeds():\n",
        "    random.seed(random_seed)\n",
        "    np.random.seed(random_seed)\n",
        "    tf.random.set_seed(random_seed)\n",
        "\n",
        "# Parse FCD filename to a list of two ints: (x, y) pixel coordinates.\n",
        "def fcd_filename_to_xy_ints(filename):\n",
        "    without_extension = filename.split('.')[0]\n",
        "    two_numeric_strings = without_extension.split('_')[1:3]\n",
        "    return list(map(int, two_numeric_strings))\n",
        "\n",
        "# Get image label from image file names ([x, y] as floats on [0,1])\n",
        "def fcd_normalized_xy(filename, pixels):\n",
        "    pixel_coordinates = fcd_filename_to_xy_ints(filename)\n",
        "    return pixel_coordinates / (np.array(pixels.shape)[1:2] / input_scale)\n",
        "\n",
        "# Draw a training image on the log. First arg is either a 24 bit RGB pixel\n",
        "# representation as read from file, or the rescaled 3xfloat used internally.\n",
        "# Optionally draw crosshairs to show center of disk.\n",
        "def draw_image(rgb_pixel_tensor, center=(0,0)):\n",
        "    i24bit = []\n",
        "\n",
        "    # 20211218\n",
        "    # if (rgb_pixel_tensor.dtype == np.double):\n",
        "\n",
        "    # 20211221\n",
        "    # print('rgb_pixel_tensor.dtype =', rgb_pixel_tensor.dtype)\n",
        "    # print('center =', center)\n",
        "\n",
        "    # if (rgb_pixel_tensor.dtype == np.float32):\n",
        "    if ((rgb_pixel_tensor.dtype == np.float32) or\n",
        "        (rgb_pixel_tensor.dtype == np.float32)):\n",
        "        unscaled_pixels = np.interp(rgb_pixel_tensor, [0, 1], [0, 255])\n",
        "        i24bit = Image.fromarray(unscaled_pixels.astype('uint8'), mode='RGB')\n",
        "    else:\n",
        "        i24bit = Image.fromarray(rgb_pixel_tensor)\n",
        "    plt.imshow(i24bit)\n",
        "\n",
        "    # 20211221\n",
        "    # if (center != (0,0)):\n",
        "    #     draw_crosshairs(center)\n",
        "    if ((center[0] != 0) or (center[1] != 0)):\n",
        "        draw_crosshairs(center)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Draw crosshairs to indicate disk position (label or estimate).\n",
        "def draw_crosshairs(center):\n",
        "    m = fcd_image_size - 1       # max image coordinate\n",
        "    s = fcd_disk_size * 1.2 / 2  # gap size (radius)\n",
        "    h = center[0] * m            # center x in pixels\n",
        "    v = center[1] * m            # center y in pixels\n",
        "    plt.hlines(v, 0, max(0, h - s), color=\"black\")\n",
        "    plt.hlines(v, min(m, h + s), m, color=\"black\")\n",
        "    plt.vlines(h, 0, max(0, v - s), color=\"white\")\n",
        "    plt.vlines(h, min(m, v + s), m, color=\"white\")\n",
        "\n",
        "# Draw line in plot between arbitrary points in plot.\n",
        "# eg: draw_line((100, 100), (924, 924), color=\"yellow\")\n",
        "def draw_line(p1, p2, color=\"white\"):\n",
        "    plt.plot([p1[0], p2[0]], [p1[1], p2[1]], color)\n",
        "\n",
        "# debug_print('fcd_filename_to_xy_ints(\"foobar_123_456\")')\n",
        "# debug_print('fcd_normalized_xy(\"foobar_123_456\", np.zeros((1024,1024,3)))')\n",
        "# debug_print('[123/(1024/input_scale), 456/(1024/input_scale)]')\n",
        "\n",
        "def timestamp_string():\n",
        "    # Just assert that we want to use Pacific time, for the benefit of cwr.\n",
        "    # The Colab server seems to think local time is UTC.\n",
        "    return datetime.now(pytz.timezone('US/Pacific')).strftime('%Y%m%d_%H%M')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyhUYHixCheD"
      },
      "source": [
        "# Data reader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "e5Bdh3inPGRM"
      },
      "outputs": [],
      "source": [
        "# # Loads FCD training data image files from \"directory_pathname\". Returns an\n",
        "# # array of images and an array of labels (each an XY pair, the relative position\n",
        "# # of the disk center). Optional \"image_count\" can limit the number of images\n",
        "# # read, by taking a random sample of availble image files, defaults to \"all\".\n",
        "\n",
        "# def read_fcd_data_from_directory(directory_pathname, image_count = 'all'):\n",
        "#     directory_contents = listdir(directory_pathname)\n",
        "#     if (image_count == 'all'): image_count = len(directory_contents)\n",
        "#     assert image_count <= len(directory_contents), \"Too few images in directory\"\n",
        "#     directory_contents = random.sample(directory_contents, image_count)\n",
        "#     image_count *= amplification # for rot/mir\n",
        "#     print('Reading', max_input_images, 'images from ' + fcd_ts_dir)\n",
        "#     print('With an amplification factor of', amplification,\n",
        "#           'for a total of', image_count, 'images in dataset.')\n",
        "\n",
        "#     # 20211218\n",
        "\n",
        "#     # # Pre-allocate a tensor for all image data and one for all labels.\n",
        "#     # local_images = np.zeros([image_count, fcd_image_size, fcd_image_size, 3])\n",
        "#     # local_labels = np.zeros([image_count, 2])\n",
        "\n",
        "#     # Pre-allocate a tensor for all image data and one for all labels.\n",
        "#     local_images = []\n",
        "#     local_labels = []\n",
        "\n",
        "#     image_index = 0\n",
        "#     for filename in tqdm(directory_contents):\n",
        "#         image_pathname = join(directory_pathname, filename)\n",
        "#         # Numpy pixel array of image object.\n",
        "#         image = Image.open(image_pathname)\n",
        "#         new_size = (fcd_image_size, fcd_image_size)\n",
        "\n",
        "#         # 20211218\n",
        "#         # pixels = asarray(image.resize(new_size, PIL.Image.LANCZOS))\n",
        "#         pixels = asarray(image.resize(new_size, PIL.Image.LANCZOS),\n",
        "#                          dtype=np.float32)\n",
        "        \n",
        "#         # Convert input image data to floating-point.\n",
        "#         float_pixels = keras_backend.cast_to_floatx(pixels)\n",
        "#         # Scale input image data to range [0, 1] (in DLAVA was [-1, 1])\n",
        "#         scaled_pixels = np.interp(float_pixels, [0, 255], [0, 1])\n",
        "#         # Read disk center position from file name.\n",
        "#         center_position = fcd_normalized_xy(filename, pixels)\n",
        "#         def center_rot90(cp): return (cp[1], 0.5 - (cp[0] - 0.5))\n",
        "#         def center_flip(cp): return (0.5 - (cp[0] - 0.5), cp[1])\n",
        "#         variations = 8  # 4 from rotations times two from mirroring\n",
        "#         assert ((amplification > 0) and (amplification <= variations))\n",
        "#         keepers = random.sample(range(1, variations), amplification - 1)\n",
        "#         keepers.append(0)\n",
        "#         for i in range(variations):\n",
        "#             if (keepers.count(i) > 0):\n",
        "\n",
        "#                 # 20211218\n",
        "\n",
        "#                 # # Copy pixel data into slice \"image_index\" of \"local_images\"\n",
        "#                 # local_images[image_index, :, :, :] = scaled_pixels\n",
        "#                 # # Copy disk center XY position into slice of \"local_labels\".\n",
        "#                 # local_labels[image_index, :] = center_position\n",
        "\n",
        "#                 # Append pixel data to \"local_images\"\n",
        "#                 local_images.append(scaled_pixels)\n",
        "#                 # Append disk center XY position to \"local_labels\".\n",
        "#                 local_labels.append(center_position)\n",
        "\n",
        "#                 image_index += 1\n",
        "#                 draw_frequency = 50 * amplification\n",
        "#                 # 20211216 is this using up too much memory (for amp=8)?\n",
        "#                 # if ((image_index % draw_frequency) == draw_frequency - 1):\n",
        "#                 #     draw_image(scaled_pixels, center_position)\n",
        "#                 #     print(image_index + 1, \"of\", image_count, \"images...\")\n",
        "#             if (i < 7):\n",
        "#                 if (i == 3):\n",
        "#                     scaled_pixels = np.flip(scaled_pixels, axis=1)\n",
        "#                     center_position = center_flip(center_position)\n",
        "#                 else:\n",
        "#                     scaled_pixels = np.rot90(scaled_pixels, k=1, axes=(0, 1))\n",
        "#                     center_position = center_rot90(center_position)\n",
        "#     return local_images, local_labels\n",
        "\n",
        "# Loads FCD training data image files from \"directory_pathname\". Returns an\n",
        "# array of images and an array of labels (each an XY pair, the relative position\n",
        "# of the disk center). Optional \"image_count\" can limit the number of images\n",
        "# read, by taking a random sample of availble image files, defaults to \"all\".\n",
        "\n",
        "def read_fcd_data_from_directory(directory_pathname, image_count = 'all'):\n",
        "    directory_contents = listdir(directory_pathname)\n",
        "    if (image_count == 'all'): image_count = len(directory_contents)\n",
        "    assert image_count <= len(directory_contents), \"Too few images in directory\"\n",
        "    directory_contents = random.sample(directory_contents, image_count)\n",
        "    image_count *= amplification # for rot/mir\n",
        "    print('Reading', max_input_images, 'images from ' + fcd_ts_dir)\n",
        "    print('With an amplification factor of', amplification,\n",
        "          'for a total of', image_count, 'images in dataset.')\n",
        "    \n",
        "    # 20211222\n",
        "    # Empty lists to collect all image data and all labels.\n",
        "    # local_images = []\n",
        "    # local_labels = []\n",
        "    # image_index = 0\n",
        "    image_list = []\n",
        "    label_list = []\n",
        "\n",
        "\n",
        "    for filename in tqdm(directory_contents):\n",
        "        image_pathname = join(directory_pathname, filename)\n",
        "        # Numpy pixel array of image object.\n",
        "        image = Image.open(image_pathname)\n",
        "        new_size = (fcd_image_size, fcd_image_size)\n",
        "        pixels = asarray(image.resize(new_size, PIL.Image.LANCZOS),\n",
        "                         dtype=np.float32)        \n",
        "        # Convert input image data to floating-point.\n",
        "        float_pixels = keras_backend.cast_to_floatx(pixels)\n",
        "        # Scale input image data to range [0, 1] (in DLAVA was [-1, 1])\n",
        "        \n",
        "        # 20211221\n",
        "        # scaled_pixels = np.interp(float_pixels, [0, 255], [0, 1])\n",
        "        scaled_pixels = np.interp(float_pixels, [0, 255], [0, 1]).astype(np.float32)\n",
        "\n",
        "        # Read disk center position from file name.\n",
        "        center_position = fcd_normalized_xy(filename, pixels)\n",
        "        def center_rot90(cp): return (cp[1], 0.5 - (cp[0] - 0.5))\n",
        "        def center_flip(cp): return (0.5 - (cp[0] - 0.5), cp[1])\n",
        "\n",
        "        variations = 8  # 4 from rotations times two from mirroring\n",
        "\n",
        "        # 20211221\n",
        "        if (allow_amp_by_inv):\n",
        "            variations *= 2  # times two for invert intensity \n",
        "\n",
        "        assert ((amplification > 0) and (amplification <= variations))\n",
        "        keepers = random.sample(range(1, variations), amplification - 1)\n",
        "        keepers.append(0)\n",
        "        for i in range(variations):\n",
        "\n",
        "            # 20211221\n",
        "            # print('i =', i)\n",
        "            # print('keepers =', keepers)\n",
        "            # print('keepers.count(i) =', keepers.count(i))\n",
        "\n",
        "            # 20211221\n",
        "            scaled_pixels = scaled_pixels.astype(np.float32)\n",
        "\n",
        "            if (keepers.count(i) > 0):\n",
        "\n",
        "                # 20211221\n",
        "                # draw_image(scaled_pixels, center_position)\n",
        "\n",
        "                # 20211222\n",
        "                # # Append pixel data to \"local_images\"\n",
        "                # local_images.append(scaled_pixels)\n",
        "                # Append pixel data to \"image_list\"\n",
        "                image_list.append(scaled_pixels)\n",
        "                # # Append disk center XY position to \"local_labels\".\n",
        "                # local_labels.append(center_position)\n",
        "                # Append disk center XY position to \"label_list\".\n",
        "                label_list.append(center_position)\n",
        "                # image_index += 1\n",
        "                # draw_frequency = 50 * amplification\n",
        "            # if (i < 7):\n",
        "            # if (i < (variations - 1)):\n",
        "            #     if (i == 3):\n",
        "            #         scaled_pixels = np.flip(scaled_pixels, axis=1)\n",
        "            #         center_position = center_flip(center_position)\n",
        "            #     else:\n",
        "            #         scaled_pixels = np.rot90(scaled_pixels, k=1, axes=(0, 1))\n",
        "            #         center_position = center_rot90(center_position)\n",
        "            #     if (i == 7):\n",
        "            #         scaled_pixels = 1 - scaled_pixels\n",
        "            if (i < (variations - 1)):\n",
        "                if (i == 7):\n",
        "                    scaled_pixels = 1 - scaled_pixels\n",
        "                else:\n",
        "                    # 20211222 I think this was not flipping the inverted ones.\n",
        "                    # if (i == 3):\n",
        "                    if ((i == 3) or (i == 3 + 8)):\n",
        "                        scaled_pixels = np.flip(scaled_pixels, axis=1)\n",
        "                        center_position = center_flip(center_position)\n",
        "                    else:\n",
        "                        scaled_pixels = np.rot90(scaled_pixels, k=1, axes=(0, 1))\n",
        "                        center_position = center_rot90(center_position)\n",
        "    # 20211222\n",
        "    # return local_images, local_labels\n",
        "    return image_list, label_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBZiONqEC7Mq"
      },
      "source": [
        "# Distance-based loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "tGGenXdUprZY"
      },
      "outputs": [],
      "source": [
        "# class FCDPositionLoss(Loss):\n",
        "#     # def __init__(self, regularization_factor=0.1, name=\"custom_mse\"):\n",
        "#     def __init__(self,\n",
        "#                  # diameter = fcd_image_size / fcd_disk_size,\n",
        "#                  diameter = float(fcd_disk_size) / float(fcd_image_size),\n",
        "#                  name = \"custom_mse\"):\n",
        "#         super().__init__(name=\"fcd_position_loss\")\n",
        "#         self.diameter = diameter\n",
        "\n",
        "#     def call(self, y_true, y_pred):\n",
        "#         # mse = tf.math.reduce_mean(tf.square(y_true - y_pred))\n",
        "#         # reg = tf.math.reduce_mean(tf.square(0.5 - y_pred))\n",
        "#         # return mse + reg * self.regularization_factor\n",
        "#         # return fcd_position_loss_helper(y_true, y_pred)\n",
        "#         return corresponding_distances(y_true, y_pred)\n",
        "\n",
        "# class FCDDiskShapedLoss(Loss):\n",
        "#     # def __init__(self, regularization_factor=0.1, name=\"custom_mse\"):\n",
        "#     def __init__(self,\n",
        "#                  # diameter = fcd_image_size / fcd_disk_size,\n",
        "#                 #  diameter = float(fcd_disk_size) / float(fcd_image_size),\n",
        "#                  radius = (float(fcd_disk_size) / float(fcd_image_size)) / 2,\n",
        "#                  name = \"custom_mse\"):\n",
        "#         super().__init__(name=\"fcd_disk_shaped_loss\")\n",
        "#         self.radius = radius\n",
        "\n",
        "#     def call(self, y_true, y_pred):\n",
        "#         # d = corresponding_distances(y_true, y_pred)\n",
        "#         # print(\"d\", d)\n",
        "#         # scaled = (d / self.radius)\n",
        "#         # print(\"scaled\", scaled)\n",
        "#         # exponentiated = scaled ** 4\n",
        "#         # print(\"exponentiated\", exponentiated)\n",
        "#         # return exponentiated\n",
        "#         # return fcd_disk_shaped_loss_helper(self.radius, y_true, y_pred)\n",
        "#         return fcd_disk_shaped_loss_helper(y_true, y_pred, self.radius)\n",
        "\n",
        "# Calculates RELATIVE disk radius on the fly -- rewrite later.\n",
        "def fcd_disk_radius():\n",
        "    return (float(fcd_disk_size) / float(fcd_image_size)) / 2\n",
        "\n",
        "def fcd_disk_shaped_loss_helper(y_true, y_pred):\n",
        "    radius = fcd_disk_radius()\n",
        "    d = corresponding_distances(y_true, y_pred)\n",
        "    # print(\"d\", d)\n",
        "    scaled = d / radius\n",
        "    # print(\"scaled\", scaled)\n",
        "    exponentiated = scaled ** 4\n",
        "    # print(\"exponentiated\", exponentiated)\n",
        "    return exponentiated\n",
        "\n",
        "# Given two tensors of 2d point coordinates, return a tensor of the Cartesian\n",
        "# distance between corresponding points in the input tensors.\n",
        "def corresponding_distances(y_true, y_pred):\n",
        "    true_pos_x, true_pos_y = tf.split(y_true, num_or_size_splits=2, axis=1)\n",
        "    pred_pos_x, pred_pos_y = tf.split(y_pred, num_or_size_splits=2, axis=1)\n",
        "    dx = true_pos_x - pred_pos_x\n",
        "    dy = true_pos_y - pred_pos_y\n",
        "    distances = tf.sqrt(tf.square(dx) + tf.square(dy))\n",
        "    return distances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-ljSOcNNU31",
        "outputId": "fadde945-08ab-4502-d970-872811f1976b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(4, 1), dtype=float32, numpy=\n",
              "array([[ 1.0995128],\n",
              "       [17.592115 ],\n",
              "       [ 0.       ],\n",
              "       [89.06067  ]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# Prototype metric to measure the fraction of predictions that are inside disks.\n",
        "# For each pair of 2d points of input, output tensor is 1 for IN and 0 for OUT.\n",
        "# def fcd_prediction_inside_disk(y_true, y_pred):\n",
        "\n",
        "# (make name shorter so it is easier to read fit() log.)\n",
        "def in_disk(y_true, y_pred):\n",
        "    distances = corresponding_distances(y_true, y_pred)\n",
        "    # relative_disk_radius = (float(fcd_disk_size) / float(fcd_image_size)) / 2\n",
        "\n",
        "    # From https://stackoverflow.com/a/42450565/1991373\n",
        "    # Boolean tensor marking where distances are less than relative_disk_radius.\n",
        "    # insides = tf.less(distances, relative_disk_radius)\n",
        "    insides = tf.less(distances, fcd_disk_radius())\n",
        "    map_to_zero_or_one = tf.cast(insides, tf.int32)\n",
        "    return map_to_zero_or_one\n",
        "\n",
        "\n",
        "example_true_positions = tf.convert_to_tensor([[1.0, 2.0],\n",
        "                                               [3.0, 4.0],\n",
        "                                               [5.0, 6.0],\n",
        "                                               [7.0, 8.0]])\n",
        "example_pred_positions = tf.convert_to_tensor([[1.1, 2.0],\n",
        "                                               [3.0, 4.2],\n",
        "                                            #    [5.0, 6.1],\n",
        "                                               [5.0, 6.0],\n",
        "                                               [7.3, 8.0]])\n",
        "\n",
        "in_disk(example_true_positions, example_pred_positions)\n",
        "fcd_disk_shaped_loss_helper(example_true_positions, example_pred_positions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgVOfEMvGF30"
      },
      "source": [
        "# Keras model utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "L0rTXiq-3su_"
      },
      "outputs": [],
      "source": [
        "# # Construct a Keras model with CNN layers at the front, striding down in\n",
        "# # resolution, then dense layers funneling down to just two output neurons\n",
        "# # representing the predicted image position center of the conspicuous disk.\n",
        "# # (First version cribbed from DLAVA chapter B3, Listing B3-41)\n",
        "\n",
        "# # def make_striding_cnn_model():\n",
        "# def make_fcd_cnn_model():\n",
        "#     cnn_act = 'relu'\n",
        "#     dense_act = 'relu'\n",
        "#     output_act = 'linear'\n",
        "#     cnn_dropout = 0.2\n",
        "#     dense_dropout = 0.5  # ala Hinton (2012)\n",
        "#     model = Sequential()\n",
        "#     # Two units of:\n",
        "#     #     2 CNN layers with 3x3 32 filters, second one striding down by half\n",
        "#     #     followed by dropout\n",
        "#     #\n",
        "#     # 20211215 maybe use 5x5 for first of each layer? more drop out?\n",
        "#     # 20211217 change 3x3/3x3 to 5x5/3x3, now try 7x7/5x5\n",
        "#     #          temporarily turn off dropout\n",
        "#     #          temporarily switch back to MSE\n",
        "#     #          go back to 5x5/3x3\n",
        "#     #          add back all dropout\n",
        "#     #          add dropout to next 2 dense layers (was 512, now add 128 and 32)\n",
        "#     #          nope, remove that.\n",
        "#     #          try doubling the number of filters in the CNN layer 32 to 64.\n",
        "#     #          nope, remove that.\n",
        "#     #          In the 2 CNN groups, add a second 3x3 CNN/dropout between other 2\n",
        "\n",
        "#     # model.add(Conv2D(32, (3, 3), activation=cnn_act, padding='same',\n",
        "#     model.add(Conv2D(32, (5, 5), activation=cnn_act, padding='same',\n",
        "#     # model.add(Conv2D(64, (5, 5), activation=cnn_act, padding='same',\n",
        "#     # model.add(Conv2D(32, (7, 7), activation=cnn_act, padding='same',\n",
        "#                      kernel_constraint=MaxNorm(3),\n",
        "#                      input_shape=(fcd_image_size, fcd_image_size, 3)))\n",
        "#     # model.add(Dropout(cnn_dropout)) # added 20211215 9:30 am ###################\n",
        "#     model.add(Dropout(cnn_dropout))\n",
        "\n",
        "#     # 20211217 4:45pm add a second 3x3 CNN/dropout between other 2:\n",
        "#     model.add(Conv2D(32, (3, 3), activation=cnn_act, padding='same',\n",
        "#                      kernel_constraint=MaxNorm(3)))\n",
        "#     model.add(Dropout(cnn_dropout))\n",
        "\n",
        "#     model.add(Conv2D(32, (3, 3), activation=cnn_act, padding='same',\n",
        "#     # model.add(Conv2D(64, (3, 3), activation=cnn_act, padding='same',\n",
        "#     # model.add(Conv2D(32, (5, 5), activation=cnn_act, padding='same',\n",
        "#                      strides=(2, 2), kernel_constraint=MaxNorm(3)))\n",
        "#     # model.add(Dropout(cnn_dropout)) # removed 20211215 9:55 am ###############\n",
        "#     model.add(Dropout(cnn_dropout))\n",
        "\n",
        "#     # model.add(Conv2D(32, (3, 3), activation=cnn_act, padding='same', \n",
        "#     model.add(Conv2D(32, (5, 5), activation=cnn_act, padding='same', \n",
        "#     # model.add(Conv2D(64, (5, 5), activation=cnn_act, padding='same', \n",
        "#     # model.add(Conv2D(32, (7, 7), activation=cnn_act, padding='same', \n",
        "#                      kernel_constraint=MaxNorm(3)))\n",
        "#     # model.add(Dropout(cnn_dropout)) # added 20211215 9:30 am ###################\n",
        "#     model.add(Dropout(cnn_dropout))\n",
        "\n",
        "#     # 20211217 4:45pm add a second 3x3 CNN/dropout between other 2:\n",
        "#     model.add(Conv2D(32, (3, 3), activation=cnn_act, padding='same',\n",
        "#                      kernel_constraint=MaxNorm(3)))\n",
        "#     model.add(Dropout(cnn_dropout))\n",
        "\n",
        "#     model.add(Conv2D(32, (3, 3), activation=cnn_act, padding='same', \n",
        "#     # model.add(Conv2D(64, (3, 3), activation=cnn_act, padding='same', \n",
        "#     # model.add(Conv2D(32, (5, 5), activation=cnn_act, padding='same', \n",
        "#                      strides=(2, 2), kernel_constraint=MaxNorm(3)))\n",
        "#     # model.add(Dropout(cnn_dropout)) # removed 20211215 9:55 am ###############\n",
        "#     model.add(Dropout(cnn_dropout))\n",
        "\n",
        "#     # 20211215 double largest size, decrease by factor of 4, etc.\n",
        "#     # try again at 10:50am, with dropout of 0.3 instead of 0.5\n",
        "\n",
        "#     # Then flatten and use a large-ish dense layer with heavy dropout.\n",
        "#     model.add(Flatten())\n",
        "#     model.add(Dense(512, activation=dense_act))\n",
        "    \n",
        "#     # model.add(Dropout(dense_dropout))\n",
        "#     # model.add(Dropout(0.3))\n",
        "#     model.add(Dropout(dense_dropout))\n",
        "\n",
        "#     # Then funnel down to two output neurons for (x, y) of predicted center.\n",
        "#     model.add(Dense(128, activation=dense_act))\n",
        "#     # model.add(Dropout(dense_dropout))\n",
        "#     model.add(Dense(32, activation=dense_act))\n",
        "#     # model.add(Dropout(dense_dropout))\n",
        "#     model.add(Dense(8, activation=dense_act))\n",
        "#     model.add(Dense(2, activation=output_act))\n",
        "\n",
        "#     # Compile with disk-shaped loss, tracking accuracy and fraction-inside-disk.\n",
        "#     # model.compile(loss=fcd_disk_shaped_loss_helper,\n",
        "#     model.compile(loss='mse',\n",
        "#                   optimizer='adam', metrics=[\"accuracy\", in_disk])\n",
        "#     return model\n",
        "\n",
        "# # 20211219 experiments:\n",
        "# #     First make cnn_filters a named parameter, then change from 32 to 48\n",
        "# #     Then reduce it to 16, while adding yet another CNN layer in each \"unit\"\n",
        "# #     Then set cnn_filters to 64 and reduce each unit to the (original) 2 CNNs.\n",
        "\n",
        "\n",
        "# # Construct a Keras model with CNN layers at the front, striding down in\n",
        "# # resolution, then dense layers funneling down to just two output neurons\n",
        "# # representing the predicted image position center of the conspicuous disk.\n",
        "# # (First version cribbed from DLAVA chapter B3, Listing B3-41)\n",
        "\n",
        "# def make_fcd_cnn_model():\n",
        "#     cnn_act = 'relu'\n",
        "#     dense_act = 'relu'\n",
        "#     output_act = 'linear'\n",
        "#     cnn_dropout = 0.2\n",
        "#     dense_dropout = 0.5  # ala Hinton (2012)\n",
        "#     # cnn_filters = 32\n",
        "#     # cnn_filters = 48\n",
        "#     # cnn_filters = 16\n",
        "#     cnn_filters = 64\n",
        "\n",
        "#     model = Sequential()\n",
        "#     # Two units of:\n",
        "#     #     ...rewrite...\n",
        "#     model.add(Conv2D(cnn_filters, (5, 5), activation=cnn_act, padding='same',\n",
        "#                      kernel_constraint=MaxNorm(3),\n",
        "#                      input_shape=(fcd_image_size, fcd_image_size, 3)))\n",
        "#     model.add(Dropout(cnn_dropout))\n",
        "#     # ###\n",
        "#     # # 20211219 11:50 duplicate\n",
        "#     # model.add(Conv2D(cnn_filters, (3, 3), activation=cnn_act, padding='same',\n",
        "#     #                  kernel_constraint=MaxNorm(3)))\n",
        "#     # model.add(Dropout(cnn_dropout))\n",
        "#     # model.add(Conv2D(cnn_filters, (3, 3), activation=cnn_act, padding='same',\n",
        "#     #                  kernel_constraint=MaxNorm(3)))\n",
        "#     # model.add(Dropout(cnn_dropout))\n",
        "#     # ###\n",
        "#     model.add(Conv2D(cnn_filters, (3, 3), activation=cnn_act, padding='same',\n",
        "#                      strides=(2, 2), kernel_constraint=MaxNorm(3)))\n",
        "#     model.add(Dropout(cnn_dropout))\n",
        "\n",
        "#     model.add(Conv2D(cnn_filters, (5, 5), activation=cnn_act, padding='same', \n",
        "#                      kernel_constraint=MaxNorm(3)))\n",
        "#     model.add(Dropout(cnn_dropout))\n",
        "#     # ###\n",
        "#     # # 20211219 11:50 duplicate\n",
        "#     # model.add(Conv2D(cnn_filters, (3, 3), activation=cnn_act, padding='same',\n",
        "#     #                  kernel_constraint=MaxNorm(3)))\n",
        "#     # model.add(Dropout(cnn_dropout))\n",
        "#     # model.add(Conv2D(cnn_filters, (3, 3), activation=cnn_act, padding='same',\n",
        "#     #                  kernel_constraint=MaxNorm(3)))\n",
        "#     # model.add(Dropout(cnn_dropout))\n",
        "#     # ###\n",
        "#     model.add(Conv2D(cnn_filters, (3, 3), activation=cnn_act, padding='same', \n",
        "#                      strides=(2, 2), kernel_constraint=MaxNorm(3)))\n",
        "#     model.add(Dropout(cnn_dropout))\n",
        "\n",
        "#     # Then flatten and use a large-ish dense layer with heavy dropout.\n",
        "#     model.add(Flatten())\n",
        "#     model.add(Dense(512, activation=dense_act))\n",
        "#     model.add(Dropout(dense_dropout))\n",
        "\n",
        "#     # Then funnel down to two output neurons for (x, y) of predicted center.\n",
        "#     model.add(Dense(128, activation=dense_act))\n",
        "#     # model.add(Dropout(dense_dropout))\n",
        "#     model.add(Dense(32, activation=dense_act))\n",
        "#     # model.add(Dropout(dense_dropout))\n",
        "#     model.add(Dense(8, activation=dense_act))\n",
        "#     model.add(Dense(2, activation=output_act))\n",
        "\n",
        "#     # Compile with mse loss, tracking accuracy and fraction-inside-disk.\n",
        "#     model.compile(loss='mse', optimizer='adam', metrics=[\"accuracy\", in_disk])\n",
        "#     return model\n",
        "\n",
        "# 20211219 experiments:\n",
        "#     First make cnn_filters a named parameter, then change from 32 to 48\n",
        "#     Then reduce it to 16, while adding yet another CNN layer in each \"unit\"\n",
        "#     Then set cnn_filters to 64 and reduce each unit to the (original) 2 CNNs.\n",
        "\n",
        "# 20211220:\n",
        "#     back to 48 filters and three CNNs per unit.\n",
        "\n",
        "# # Construct a Keras model with CNN layers at the front, striding down in\n",
        "# # resolution, then dense layers funneling down to just two output neurons\n",
        "# # representing the predicted image position center of the conspicuous disk.\n",
        "# # (First version cribbed from DLAVA chapter B3, Listing B3-41)\n",
        "\n",
        "# def make_fcd_cnn_model():\n",
        "#     cnn_act = 'relu'\n",
        "#     dense_act = 'relu'\n",
        "#     output_act = 'linear'\n",
        "#     cnn_dropout = 0.2\n",
        "#     dense_dropout = 0.5  # ala Hinton (2012)\n",
        "#     # cnn_filters = 32\n",
        "#     cnn_filters = 48\n",
        "#     # cnn_filters = 16\n",
        "#     # cnn_filters = 64\n",
        "\n",
        "#     model = Sequential()\n",
        "#     # Two units of:\n",
        "#     #     ...rewrite...\n",
        "#     model.add(Conv2D(cnn_filters, (5, 5), activation=cnn_act, padding='same',\n",
        "#                      kernel_constraint=MaxNorm(3),\n",
        "#                      input_shape=(fcd_image_size, fcd_image_size, 3)))\n",
        "#     model.add(Dropout(cnn_dropout))\n",
        "#     # ###\n",
        "#     # # 20211219 11:50 duplicate\n",
        "#     model.add(Conv2D(cnn_filters, (3, 3), activation=cnn_act, padding='same',\n",
        "#                      kernel_constraint=MaxNorm(3)))\n",
        "#     model.add(Dropout(cnn_dropout))\n",
        "#     # model.add(Conv2D(cnn_filters, (3, 3), activation=cnn_act, padding='same',\n",
        "#     #                  kernel_constraint=MaxNorm(3)))\n",
        "#     # model.add(Dropout(cnn_dropout))\n",
        "#     # ###\n",
        "#     model.add(Conv2D(cnn_filters, (3, 3), activation=cnn_act, padding='same',\n",
        "#                      strides=(2, 2), kernel_constraint=MaxNorm(3)))\n",
        "#     model.add(Dropout(cnn_dropout))\n",
        "\n",
        "#     model.add(Conv2D(cnn_filters, (5, 5), activation=cnn_act, padding='same', \n",
        "#                      kernel_constraint=MaxNorm(3)))\n",
        "#     model.add(Dropout(cnn_dropout))\n",
        "#     # ###\n",
        "#     # # 20211219 11:50 duplicate\n",
        "#     model.add(Conv2D(cnn_filters, (3, 3), activation=cnn_act, padding='same',\n",
        "#                      kernel_constraint=MaxNorm(3)))\n",
        "#     model.add(Dropout(cnn_dropout))\n",
        "#     # model.add(Conv2D(cnn_filters, (3, 3), activation=cnn_act, padding='same',\n",
        "#     #                  kernel_constraint=MaxNorm(3)))\n",
        "#     # model.add(Dropout(cnn_dropout))\n",
        "#     # ###\n",
        "#     model.add(Conv2D(cnn_filters, (3, 3), activation=cnn_act, padding='same', \n",
        "#                      strides=(2, 2), kernel_constraint=MaxNorm(3)))\n",
        "#     model.add(Dropout(cnn_dropout))\n",
        "\n",
        "#     # Then flatten and use a large-ish dense layer with heavy dropout.\n",
        "#     model.add(Flatten())\n",
        "#     model.add(Dense(512, activation=dense_act))\n",
        "#     model.add(Dropout(dense_dropout))\n",
        "\n",
        "#     # Then funnel down to two output neurons for (x, y) of predicted center.\n",
        "#     model.add(Dense(128, activation=dense_act))\n",
        "#     # model.add(Dropout(dense_dropout))\n",
        "#     model.add(Dense(32, activation=dense_act))\n",
        "#     # model.add(Dropout(dense_dropout))\n",
        "#     model.add(Dense(8, activation=dense_act))\n",
        "#     model.add(Dense(2, activation=output_act))\n",
        "\n",
        "#     # Compile with mse loss, tracking accuracy and fraction-inside-disk.\n",
        "#     model.compile(loss='mse', optimizer='adam', metrics=[\"accuracy\", in_disk])\n",
        "#     return model\n",
        "\n",
        "# Construct a Keras model with CNN layers at the front, striding down in\n",
        "# resolution, then dense layers funneling down to just two output neurons\n",
        "# representing the predicted image position center of the conspicuous disk.\n",
        "# (First version cribbed from DLAVA chapter B3, Listing B3-41)\n",
        "\n",
        "# 20211220:\n",
        "#     back to 48 filters and three CNNs per unit.\n",
        "#     increase cnn_dropout from 0.2 to 0.3, add dropout for Dense(128)\n",
        "\n",
        "def make_fcd_cnn_model():\n",
        "    cnn_act = 'relu'\n",
        "    dense_act = 'relu'\n",
        "    output_act = 'linear'\n",
        "    cnn_dropout = 0.2\n",
        "    dense_dropout = 0.5  # ala Hinton (2012)\n",
        "    # cnn_filters = 48\n",
        "    cnn_filters = 32\n",
        "\n",
        "    model = Sequential()\n",
        "    # Two units of:\n",
        "    #     ...rewrite...\n",
        "    model.add(Conv2D(cnn_filters, (5, 5), activation=cnn_act, padding='same',\n",
        "                     kernel_constraint=MaxNorm(3),\n",
        "                     input_shape=(fcd_image_size, fcd_image_size, 3)))\n",
        "    model.add(Dropout(cnn_dropout))\n",
        "    model.add(Conv2D(cnn_filters, (3, 3), activation=cnn_act, padding='same',\n",
        "                     kernel_constraint=MaxNorm(3)))\n",
        "    model.add(Dropout(cnn_dropout))\n",
        "    model.add(Conv2D(cnn_filters, (3, 3), activation=cnn_act, padding='same',\n",
        "                     strides=(2, 2), kernel_constraint=MaxNorm(3)))\n",
        "    model.add(Dropout(cnn_dropout))\n",
        "\n",
        "    model.add(Conv2D(cnn_filters, (5, 5), activation=cnn_act, padding='same', \n",
        "                     kernel_constraint=MaxNorm(3)))\n",
        "    model.add(Dropout(cnn_dropout))\n",
        "    model.add(Conv2D(cnn_filters, (3, 3), activation=cnn_act, padding='same',\n",
        "                     kernel_constraint=MaxNorm(3)))\n",
        "    model.add(Dropout(cnn_dropout))\n",
        "    model.add(Conv2D(cnn_filters, (3, 3), activation=cnn_act, padding='same', \n",
        "                     strides=(2, 2), kernel_constraint=MaxNorm(3)))\n",
        "    model.add(Dropout(cnn_dropout))\n",
        "\n",
        "    # Then flatten and use a large-ish dense layer with heavy dropout.\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(512, activation=dense_act))\n",
        "    model.add(Dropout(dense_dropout))\n",
        "\n",
        "    # Then funnel down to two output neurons for (x, y) of predicted center.\n",
        "    model.add(Dense(128, activation=dense_act))\n",
        "    # model.add(Dropout(dense_dropout))\n",
        "    model.add(Dense(32, activation=dense_act))\n",
        "    # model.add(Dropout(dense_dropout))\n",
        "    model.add(Dense(8, activation=dense_act))\n",
        "    model.add(Dense(2, activation=output_act))\n",
        "\n",
        "    # Compile with mse loss, tracking accuracy and fraction-inside-disk.\n",
        "    model.compile(loss='mse', optimizer='adam', metrics=[\"accuracy\", in_disk])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "E6A5Obp7DjWr"
      },
      "outputs": [],
      "source": [
        "# Utility to fit and plot a run, again cribbed from DLAVA chapter B3.\n",
        "def run_model(model_maker, plot_title):\n",
        "    model = model_maker()\n",
        "\n",
        "    # print(\"In run_model():\")\n",
        "    # debug_print('X_train.shape')\n",
        "    # debug_print(\"y_train.shape\")\n",
        "    # 20211218\n",
        "    # history = model.fit(X_train, y_train, validation_split=0.2,\n",
        "    #                     epochs=fcd_epochs, batch_size=fcd_batch_size)\n",
        "    \n",
        "    # (X_train, X_test, y_train, y_test)\n",
        "\n",
        "    history = model.fit(X_train,\n",
        "                        y_train,\n",
        "\n",
        "                        # validation_split=0.2,\n",
        "                        validation_data = (X_test, y_test),\n",
        "                        \n",
        "                        epochs=fcd_epochs,\n",
        "                        batch_size=fcd_batch_size)\n",
        "\n",
        "\n",
        "    print()\n",
        "    plot_accuracy_and_loss(history, plot_title)\n",
        "    return model, history\n",
        "\n",
        "# A little utility to draw plots of accuracy and loss.\n",
        "def plot_accuracy_and_loss(history, plot_title):\n",
        "    xs = range(len(history.history['accuracy']))\n",
        "    # plt.figure(figsize=(10,3))\n",
        "    plt.figure(figsize=(15,3))\n",
        "\n",
        "    # plt.subplot(1, 2, 1)\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.plot(xs, history.history['accuracy'], label='train')\n",
        "    plt.plot(xs, history.history['val_accuracy'], label='validation')\n",
        "    plt.legend(loc='lower left')\n",
        "    plt.xlabel('epochs')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.title(plot_title+': Accuracy')\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    # plt.plot(xs, history.history['fcd_prediction_inside_disk'], label='train')\n",
        "    # plt.plot(xs, history.history['val_fcd_prediction_inside_disk'], label='validation')\n",
        "    plt.plot(xs, history.history['in_disk'], label='train')\n",
        "    plt.plot(xs, history.history['val_in_disk'], label='validation')\n",
        "    plt.legend(loc='lower left')\n",
        "    plt.xlabel('epochs')\n",
        "    plt.ylabel('fraction inside disk')\n",
        "    plt.title(plot_title+': fraction inside disk')\n",
        "\n",
        "    # plt.subplot(1, 2, 2)\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.plot(xs, history.history['loss'], label='train')\n",
        "    plt.plot(xs, history.history['val_loss'], label='validation')\n",
        "    plt.legend(loc='upper left')\n",
        "    plt.xlabel('epochs')\n",
        "    plt.ylabel('loss')\n",
        "    plt.title(plot_title+': Loss')\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Iot7WB8KhGg"
      },
      "source": [
        "# Read training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188,
          "referenced_widgets": [
            "05fc49e48e134b7d8006903a9a7914dc",
            "d7c5640b4f124dd09c5854d0520e15d5",
            "ee3a546c3d324d318ca11edd4d9ea519",
            "398dbdc7423a414db0f81e832f48e56a",
            "1069b4b20b9649969f0f47ee32d708c6",
            "7fb8d1a256c74f57b47c692057ba0960",
            "a7163b9342df463c8b18b1bb69117a68",
            "ba34e62c1f9b4469b28c743bb5ccdf9f",
            "665f6d76a8e148f8b7bb507fb1845ed3",
            "974c5b3247b7434da5641685993cb764",
            "bd3283343fe249b98589951a0b6411fe"
          ]
        },
        "id": "zKUhDLbgJxtR",
        "outputId": "a84be807-266a-4ab4-d89c-c31e1640c8c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading all images from /content/drive/My Drive/PredatorEye/fcd_training_set/\n",
            "With an amplification factor of 8 for a total of 40000 images in dataset.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "05fc49e48e134b7d8006903a9a7914dc",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/5000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total of 40000 labeled images.\n",
            "len(X_train) = 32000\n",
            "len(y_train) = 32000\n",
            "len(X_test) = 8000\n",
            "len(y_test) = 8000\n",
            "Elapsed time: 164 seconds.\n"
          ]
        }
      ],
      "source": [
        "# Read FCD training data from a given directory.\n",
        "reset_random_seeds()\n",
        "start_time = time.time()\n",
        "# (X_train, y_train) = ([], [])  # To release memory when rerunning in notebook.\n",
        "# To release memory when rerunning in notebook.\n",
        "(X_train, X_test, y_train, y_test) = ([], [], [], [])  \n",
        "gc.collect()\n",
        "(X_train, y_train) = read_fcd_data_from_directory(fcd_ts_dir, max_input_images)\n",
        "# print('Total of', X_train.shape[0], 'labeled images.')\n",
        "print('Total of', len(X_train), 'labeled images.')\n",
        "\n",
        "# 20211218\n",
        "(X_train, X_test, y_train, y_test) = train_test_split(X_train, y_train,\n",
        "                                                      test_size=0.2)\n",
        "print('len(X_train) =', len(X_train))\n",
        "print('len(y_train) =', len(y_train))\n",
        "print('len(X_test) =', len(X_test))\n",
        "print('len(y_test) =', len(y_test))\n",
        "\n",
        "# Convert from python lists to np array.\n",
        "X_train = np.array(X_train, dtype=np.float32)\n",
        "y_train = np.array(y_train, dtype=np.float32)\n",
        "X_test = np.array(X_test, dtype=np.float32)\n",
        "y_test = np.array(y_test, dtype=np.float32)\n",
        "\n",
        "print('Elapsed time:', int(time.time() - start_time), 'seconds.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQy5wa_GLNG4"
      },
      "source": [
        "# Build and train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ppYkZ-grG2E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2439bfa0-4647-4091-8913-c5d9050c448b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "250/250 [==============================] - 40s 124ms/step - loss: 0.0619 - accuracy: 0.5019 - in_disk: 0.0473 - val_loss: 0.0541 - val_accuracy: 0.4971 - val_in_disk: 0.0509\n",
            "Epoch 2/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0562 - accuracy: 0.5032 - in_disk: 0.0499 - val_loss: 0.0551 - val_accuracy: 0.5029 - val_in_disk: 0.0505\n",
            "Epoch 3/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0559 - accuracy: 0.4956 - in_disk: 0.0488 - val_loss: 0.0544 - val_accuracy: 0.5029 - val_in_disk: 0.0504\n",
            "Epoch 4/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0556 - accuracy: 0.5004 - in_disk: 0.0492 - val_loss: 0.0545 - val_accuracy: 0.4971 - val_in_disk: 0.0491\n",
            "Epoch 5/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0552 - accuracy: 0.5005 - in_disk: 0.0501 - val_loss: 0.0540 - val_accuracy: 0.5029 - val_in_disk: 0.0518\n",
            "Epoch 6/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0512 - accuracy: 0.5392 - in_disk: 0.0704 - val_loss: 0.0302 - val_accuracy: 0.7671 - val_in_disk: 0.2903\n",
            "Epoch 7/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0267 - accuracy: 0.7724 - in_disk: 0.3412 - val_loss: 0.0248 - val_accuracy: 0.7974 - val_in_disk: 0.4512\n",
            "Epoch 8/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0210 - accuracy: 0.8145 - in_disk: 0.4863 - val_loss: 0.0176 - val_accuracy: 0.8384 - val_in_disk: 0.6076\n",
            "Epoch 9/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0178 - accuracy: 0.8357 - in_disk: 0.5680 - val_loss: 0.0166 - val_accuracy: 0.8525 - val_in_disk: 0.6310\n",
            "Epoch 10/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0163 - accuracy: 0.8453 - in_disk: 0.6125 - val_loss: 0.0152 - val_accuracy: 0.8566 - val_in_disk: 0.6708\n",
            "Epoch 11/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0151 - accuracy: 0.8514 - in_disk: 0.6481 - val_loss: 0.0140 - val_accuracy: 0.8643 - val_in_disk: 0.7147\n",
            "Epoch 12/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0143 - accuracy: 0.8595 - in_disk: 0.6697 - val_loss: 0.0141 - val_accuracy: 0.8731 - val_in_disk: 0.7188\n",
            "Epoch 13/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0135 - accuracy: 0.8626 - in_disk: 0.6894 - val_loss: 0.0141 - val_accuracy: 0.8650 - val_in_disk: 0.7287\n",
            "Epoch 14/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0129 - accuracy: 0.8672 - in_disk: 0.7062 - val_loss: 0.0135 - val_accuracy: 0.8673 - val_in_disk: 0.7498\n",
            "Epoch 15/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0121 - accuracy: 0.8757 - in_disk: 0.7198 - val_loss: 0.0129 - val_accuracy: 0.8767 - val_in_disk: 0.7436\n",
            "Epoch 16/100\n",
            "250/250 [==============================] - 29s 116ms/step - loss: 0.0119 - accuracy: 0.8745 - in_disk: 0.7236 - val_loss: 0.0122 - val_accuracy: 0.8794 - val_in_disk: 0.7732\n",
            "Epoch 17/100\n",
            "250/250 [==============================] - 29s 116ms/step - loss: 0.0114 - accuracy: 0.8786 - in_disk: 0.7366 - val_loss: 0.0122 - val_accuracy: 0.8817 - val_in_disk: 0.7669\n",
            "Epoch 18/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0105 - accuracy: 0.8836 - in_disk: 0.7443 - val_loss: 0.0117 - val_accuracy: 0.8777 - val_in_disk: 0.7793\n",
            "Epoch 19/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0101 - accuracy: 0.8856 - in_disk: 0.7509 - val_loss: 0.0118 - val_accuracy: 0.8875 - val_in_disk: 0.7764\n",
            "Epoch 20/100\n",
            "250/250 [==============================] - 29s 116ms/step - loss: 0.0096 - accuracy: 0.8887 - in_disk: 0.7588 - val_loss: 0.0114 - val_accuracy: 0.8861 - val_in_disk: 0.7768\n",
            "Epoch 21/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0092 - accuracy: 0.8919 - in_disk: 0.7636 - val_loss: 0.0115 - val_accuracy: 0.8882 - val_in_disk: 0.7876\n",
            "Epoch 22/100\n",
            "250/250 [==============================] - 29s 116ms/step - loss: 0.0086 - accuracy: 0.8938 - in_disk: 0.7724 - val_loss: 0.0114 - val_accuracy: 0.8823 - val_in_disk: 0.7951\n",
            "Epoch 23/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0084 - accuracy: 0.8983 - in_disk: 0.7726 - val_loss: 0.0116 - val_accuracy: 0.8844 - val_in_disk: 0.7943\n",
            "Epoch 24/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0078 - accuracy: 0.9007 - in_disk: 0.7896 - val_loss: 0.0116 - val_accuracy: 0.8864 - val_in_disk: 0.7897\n",
            "Epoch 25/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0075 - accuracy: 0.9054 - in_disk: 0.7903 - val_loss: 0.0116 - val_accuracy: 0.8921 - val_in_disk: 0.8006\n",
            "Epoch 26/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0073 - accuracy: 0.9061 - in_disk: 0.7954 - val_loss: 0.0113 - val_accuracy: 0.8882 - val_in_disk: 0.8005\n",
            "Epoch 27/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0067 - accuracy: 0.9096 - in_disk: 0.8067 - val_loss: 0.0108 - val_accuracy: 0.8996 - val_in_disk: 0.8146\n",
            "Epoch 28/100\n",
            "250/250 [==============================] - 29s 116ms/step - loss: 0.0069 - accuracy: 0.9114 - in_disk: 0.8039 - val_loss: 0.0115 - val_accuracy: 0.8865 - val_in_disk: 0.7994\n",
            "Epoch 29/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0062 - accuracy: 0.9132 - in_disk: 0.8139 - val_loss: 0.0106 - val_accuracy: 0.8929 - val_in_disk: 0.8116\n",
            "Epoch 30/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0061 - accuracy: 0.9147 - in_disk: 0.8159 - val_loss: 0.0113 - val_accuracy: 0.8959 - val_in_disk: 0.8025\n",
            "Epoch 31/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0059 - accuracy: 0.9154 - in_disk: 0.8167 - val_loss: 0.0108 - val_accuracy: 0.8956 - val_in_disk: 0.8198\n",
            "Epoch 32/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0059 - accuracy: 0.9159 - in_disk: 0.8196 - val_loss: 0.0107 - val_accuracy: 0.8971 - val_in_disk: 0.8165\n",
            "Epoch 33/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0056 - accuracy: 0.9199 - in_disk: 0.8303 - val_loss: 0.0112 - val_accuracy: 0.8900 - val_in_disk: 0.8117\n",
            "Epoch 34/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0052 - accuracy: 0.9226 - in_disk: 0.8340 - val_loss: 0.0109 - val_accuracy: 0.8965 - val_in_disk: 0.8144\n",
            "Epoch 35/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0052 - accuracy: 0.9207 - in_disk: 0.8370 - val_loss: 0.0103 - val_accuracy: 0.9038 - val_in_disk: 0.8285\n",
            "Epoch 36/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0051 - accuracy: 0.9226 - in_disk: 0.8390 - val_loss: 0.0105 - val_accuracy: 0.9032 - val_in_disk: 0.8175\n",
            "Epoch 37/100\n",
            "250/250 [==============================] - 29s 116ms/step - loss: 0.0051 - accuracy: 0.9205 - in_disk: 0.8369 - val_loss: 0.0104 - val_accuracy: 0.8991 - val_in_disk: 0.8206\n",
            "Epoch 38/100\n",
            "250/250 [==============================] - 29s 116ms/step - loss: 0.0051 - accuracy: 0.9236 - in_disk: 0.8380 - val_loss: 0.0109 - val_accuracy: 0.8949 - val_in_disk: 0.8186\n",
            "Epoch 39/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0049 - accuracy: 0.9262 - in_disk: 0.8411 - val_loss: 0.0103 - val_accuracy: 0.9029 - val_in_disk: 0.8249\n",
            "Epoch 40/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0048 - accuracy: 0.9266 - in_disk: 0.8420 - val_loss: 0.0110 - val_accuracy: 0.8984 - val_in_disk: 0.8217\n",
            "Epoch 41/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0048 - accuracy: 0.9258 - in_disk: 0.8436 - val_loss: 0.0107 - val_accuracy: 0.8957 - val_in_disk: 0.8246\n",
            "Epoch 42/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0047 - accuracy: 0.9288 - in_disk: 0.8508 - val_loss: 0.0104 - val_accuracy: 0.9007 - val_in_disk: 0.8289\n",
            "Epoch 43/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0044 - accuracy: 0.9291 - in_disk: 0.8522 - val_loss: 0.0103 - val_accuracy: 0.9074 - val_in_disk: 0.8279\n",
            "Epoch 44/100\n",
            "250/250 [==============================] - 29s 116ms/step - loss: 0.0041 - accuracy: 0.9324 - in_disk: 0.8618 - val_loss: 0.0125 - val_accuracy: 0.8859 - val_in_disk: 0.7941\n",
            "Epoch 45/100\n",
            "250/250 [==============================] - 29s 116ms/step - loss: 0.0046 - accuracy: 0.9262 - in_disk: 0.8510 - val_loss: 0.0102 - val_accuracy: 0.9055 - val_in_disk: 0.8271\n",
            "Epoch 46/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0039 - accuracy: 0.9333 - in_disk: 0.8692 - val_loss: 0.0100 - val_accuracy: 0.9097 - val_in_disk: 0.8415\n",
            "Epoch 47/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0039 - accuracy: 0.9361 - in_disk: 0.8670 - val_loss: 0.0107 - val_accuracy: 0.9029 - val_in_disk: 0.8289\n",
            "Epoch 48/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0040 - accuracy: 0.9325 - in_disk: 0.8641 - val_loss: 0.0106 - val_accuracy: 0.9010 - val_in_disk: 0.8296\n",
            "Epoch 49/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0039 - accuracy: 0.9321 - in_disk: 0.8677 - val_loss: 0.0106 - val_accuracy: 0.9018 - val_in_disk: 0.8270\n",
            "Epoch 50/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0039 - accuracy: 0.9337 - in_disk: 0.8672 - val_loss: 0.0102 - val_accuracy: 0.9040 - val_in_disk: 0.8316\n",
            "Epoch 51/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0038 - accuracy: 0.9340 - in_disk: 0.8682 - val_loss: 0.0102 - val_accuracy: 0.9069 - val_in_disk: 0.8380\n",
            "Epoch 52/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0037 - accuracy: 0.9339 - in_disk: 0.8726 - val_loss: 0.0113 - val_accuracy: 0.8956 - val_in_disk: 0.8156\n",
            "Epoch 53/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0040 - accuracy: 0.9308 - in_disk: 0.8650 - val_loss: 0.0107 - val_accuracy: 0.9026 - val_in_disk: 0.8276\n",
            "Epoch 54/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0037 - accuracy: 0.9341 - in_disk: 0.8729 - val_loss: 0.0107 - val_accuracy: 0.9031 - val_in_disk: 0.8330\n",
            "Epoch 55/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0034 - accuracy: 0.9377 - in_disk: 0.8788 - val_loss: 0.0098 - val_accuracy: 0.9107 - val_in_disk: 0.8409\n",
            "Epoch 56/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0035 - accuracy: 0.9377 - in_disk: 0.8782 - val_loss: 0.0103 - val_accuracy: 0.9107 - val_in_disk: 0.8367\n",
            "Epoch 57/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0034 - accuracy: 0.9372 - in_disk: 0.8818 - val_loss: 0.0103 - val_accuracy: 0.9075 - val_in_disk: 0.8328\n",
            "Epoch 58/100\n",
            "250/250 [==============================] - 29s 116ms/step - loss: 0.0034 - accuracy: 0.9375 - in_disk: 0.8832 - val_loss: 0.0099 - val_accuracy: 0.9129 - val_in_disk: 0.8389\n",
            "Epoch 59/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0035 - accuracy: 0.9374 - in_disk: 0.8799 - val_loss: 0.0101 - val_accuracy: 0.9057 - val_in_disk: 0.8376\n",
            "Epoch 60/100\n",
            "250/250 [==============================] - 29s 116ms/step - loss: 0.0036 - accuracy: 0.9370 - in_disk: 0.8796 - val_loss: 0.0106 - val_accuracy: 0.9044 - val_in_disk: 0.8298\n",
            "Epoch 61/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0034 - accuracy: 0.9367 - in_disk: 0.8842 - val_loss: 0.0090 - val_accuracy: 0.9156 - val_in_disk: 0.8440\n",
            "Epoch 62/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0033 - accuracy: 0.9383 - in_disk: 0.8857 - val_loss: 0.0105 - val_accuracy: 0.9062 - val_in_disk: 0.8280\n",
            "Epoch 63/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0031 - accuracy: 0.9399 - in_disk: 0.8902 - val_loss: 0.0097 - val_accuracy: 0.9124 - val_in_disk: 0.8453\n",
            "Epoch 64/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0032 - accuracy: 0.9408 - in_disk: 0.8894 - val_loss: 0.0099 - val_accuracy: 0.9107 - val_in_disk: 0.8403\n",
            "Epoch 65/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0031 - accuracy: 0.9404 - in_disk: 0.8928 - val_loss: 0.0107 - val_accuracy: 0.9032 - val_in_disk: 0.8242\n",
            "Epoch 66/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0034 - accuracy: 0.9381 - in_disk: 0.8834 - val_loss: 0.0097 - val_accuracy: 0.9112 - val_in_disk: 0.8411\n",
            "Epoch 67/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0029 - accuracy: 0.9424 - in_disk: 0.8968 - val_loss: 0.0098 - val_accuracy: 0.9136 - val_in_disk: 0.8394\n",
            "Epoch 68/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0030 - accuracy: 0.9432 - in_disk: 0.8941 - val_loss: 0.0091 - val_accuracy: 0.9180 - val_in_disk: 0.8476\n",
            "Epoch 69/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0032 - accuracy: 0.9423 - in_disk: 0.8911 - val_loss: 0.0103 - val_accuracy: 0.9094 - val_in_disk: 0.8276\n",
            "Epoch 70/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0031 - accuracy: 0.9433 - in_disk: 0.8946 - val_loss: 0.0096 - val_accuracy: 0.9131 - val_in_disk: 0.8405\n",
            "Epoch 71/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0028 - accuracy: 0.9448 - in_disk: 0.9010 - val_loss: 0.0096 - val_accuracy: 0.9139 - val_in_disk: 0.8416\n",
            "Epoch 72/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0034 - accuracy: 0.9402 - in_disk: 0.8876 - val_loss: 0.0108 - val_accuracy: 0.9038 - val_in_disk: 0.8266\n",
            "Epoch 73/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0031 - accuracy: 0.9415 - in_disk: 0.8926 - val_loss: 0.0095 - val_accuracy: 0.9169 - val_in_disk: 0.8455\n",
            "Epoch 74/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0029 - accuracy: 0.9417 - in_disk: 0.9015 - val_loss: 0.0093 - val_accuracy: 0.9169 - val_in_disk: 0.8540\n",
            "Epoch 75/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0027 - accuracy: 0.9450 - in_disk: 0.9060 - val_loss: 0.0105 - val_accuracy: 0.9070 - val_in_disk: 0.8349\n",
            "Epoch 76/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0028 - accuracy: 0.9454 - in_disk: 0.9030 - val_loss: 0.0092 - val_accuracy: 0.9189 - val_in_disk: 0.8500\n",
            "Epoch 77/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0026 - accuracy: 0.9465 - in_disk: 0.9061 - val_loss: 0.0089 - val_accuracy: 0.9169 - val_in_disk: 0.8511\n",
            "Epoch 78/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0030 - accuracy: 0.9432 - in_disk: 0.8974 - val_loss: 0.0097 - val_accuracy: 0.9120 - val_in_disk: 0.8382\n",
            "Epoch 79/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0028 - accuracy: 0.9454 - in_disk: 0.9006 - val_loss: 0.0089 - val_accuracy: 0.9174 - val_in_disk: 0.8571\n",
            "Epoch 80/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0024 - accuracy: 0.9478 - in_disk: 0.9139 - val_loss: 0.0095 - val_accuracy: 0.9112 - val_in_disk: 0.8453\n",
            "Epoch 81/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0027 - accuracy: 0.9459 - in_disk: 0.9056 - val_loss: 0.0090 - val_accuracy: 0.9211 - val_in_disk: 0.8497\n",
            "Epoch 82/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0024 - accuracy: 0.9481 - in_disk: 0.9109 - val_loss: 0.0096 - val_accuracy: 0.9133 - val_in_disk: 0.8410\n",
            "Epoch 83/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0027 - accuracy: 0.9446 - in_disk: 0.9053 - val_loss: 0.0095 - val_accuracy: 0.9131 - val_in_disk: 0.8410\n",
            "Epoch 84/100\n",
            "250/250 [==============================] - 29s 116ms/step - loss: 0.0028 - accuracy: 0.9458 - in_disk: 0.9034 - val_loss: 0.0097 - val_accuracy: 0.9121 - val_in_disk: 0.8391\n",
            "Epoch 85/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0026 - accuracy: 0.9473 - in_disk: 0.9087 - val_loss: 0.0093 - val_accuracy: 0.9187 - val_in_disk: 0.8526\n",
            "Epoch 86/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0026 - accuracy: 0.9474 - in_disk: 0.9109 - val_loss: 0.0089 - val_accuracy: 0.9234 - val_in_disk: 0.8561\n",
            "Epoch 87/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0026 - accuracy: 0.9456 - in_disk: 0.9097 - val_loss: 0.0101 - val_accuracy: 0.9124 - val_in_disk: 0.8450\n",
            "Epoch 88/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0026 - accuracy: 0.9465 - in_disk: 0.9107 - val_loss: 0.0096 - val_accuracy: 0.9118 - val_in_disk: 0.8536\n",
            "Epoch 89/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0028 - accuracy: 0.9438 - in_disk: 0.9030 - val_loss: 0.0091 - val_accuracy: 0.9201 - val_in_disk: 0.8570\n",
            "Epoch 90/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0026 - accuracy: 0.9461 - in_disk: 0.9057 - val_loss: 0.0094 - val_accuracy: 0.9141 - val_in_disk: 0.8501\n",
            "Epoch 91/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0027 - accuracy: 0.9435 - in_disk: 0.9051 - val_loss: 0.0092 - val_accuracy: 0.9184 - val_in_disk: 0.8485\n",
            "Epoch 92/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0025 - accuracy: 0.9468 - in_disk: 0.9141 - val_loss: 0.0090 - val_accuracy: 0.9179 - val_in_disk: 0.8589\n",
            "Epoch 93/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0023 - accuracy: 0.9489 - in_disk: 0.9184 - val_loss: 0.0091 - val_accuracy: 0.9189 - val_in_disk: 0.8565\n",
            "Epoch 94/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0023 - accuracy: 0.9482 - in_disk: 0.9199 - val_loss: 0.0085 - val_accuracy: 0.9218 - val_in_disk: 0.8645\n",
            "Epoch 95/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0024 - accuracy: 0.9486 - in_disk: 0.9169 - val_loss: 0.0098 - val_accuracy: 0.9036 - val_in_disk: 0.8407\n",
            "Epoch 96/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0023 - accuracy: 0.9482 - in_disk: 0.9191 - val_loss: 0.0087 - val_accuracy: 0.9216 - val_in_disk: 0.8587\n",
            "Epoch 97/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0023 - accuracy: 0.9505 - in_disk: 0.9216 - val_loss: 0.0089 - val_accuracy: 0.9180 - val_in_disk: 0.8521\n",
            "Epoch 98/100\n",
            "250/250 [==============================] - 29s 117ms/step - loss: 0.0022 - accuracy: 0.9496 - in_disk: 0.9223 - val_loss: 0.0095 - val_accuracy: 0.9133 - val_in_disk: 0.8466\n",
            "Epoch 99/100\n",
            "135/250 [===============>..............] - ETA: 12s - loss: 0.0022 - accuracy: 0.9500 - in_disk: 0.9215"
          ]
        }
      ],
      "source": [
        "# Run a model.\n",
        "reset_random_seeds()\n",
        "start_time = time.time()\n",
        "fcd_model_timestamp = timestamp_string()\n",
        "(model, history) = ([], [])  # To release memory when rerunning in notebook.\n",
        "gc.collect()\n",
        "\n",
        "(model, history) = run_model(make_fcd_cnn_model, 'FCD')\n",
        "\n",
        "elapsed_seconds = int(time.time() - start_time)\n",
        "print('Elapsed time: ' + str(elapsed_seconds) + ' seconds (' +\n",
        "      str(int(elapsed_seconds / 60)) +' minutes).')\n",
        "\n",
        "model.save(model_save_directory + fcd_model_timestamp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYrWnkXtLVcO"
      },
      "source": [
        "# Analyze results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VfqgguszL0sv"
      },
      "outputs": [],
      "source": [
        "# Draw some results to understand performance\n",
        "# TODO needs to discriminate between training and validation sets.\n",
        "#      feature best/worse results of both cases?\n",
        "# 20211210 refactor to not run predict on whole training set\n",
        "def draw_results(count = 20):\n",
        "\n",
        "    # 20211218 (should be a parameter to draw_results())\n",
        "    # for i in random.sample(range(X_train.shape[0]), count) :\n",
        "    for i in random.sample(range(X_test.shape[0]), count) :\n",
        "\n",
        "        # 20211218 (should be a parameter to draw_results())\n",
        "        # pixel_tensor = X_train[i, :, :, :]\n",
        "        pixel_tensor = X_test[i, :, :, :]\n",
        "        \n",
        "        predict = model.predict(tf.convert_to_tensor([pixel_tensor]))[0]\n",
        "        x = predict[0]\n",
        "        y = predict[1]\n",
        "        print(i, \": (\", x, \",\", y, \")\")\n",
        "\n",
        "        # 20211218\n",
        "        # draw_image(X_train[i, :, :, :], [x, y])\n",
        "        draw_image(pixel_tensor, [x, y])\n",
        "\n",
        "reset_random_seeds()\n",
        "draw_results()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Find_Conspicuous_Disk.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "05fc49e48e134b7d8006903a9a7914dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d7c5640b4f124dd09c5854d0520e15d5",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ee3a546c3d324d318ca11edd4d9ea519",
              "IPY_MODEL_398dbdc7423a414db0f81e832f48e56a",
              "IPY_MODEL_1069b4b20b9649969f0f47ee32d708c6"
            ]
          }
        },
        "d7c5640b4f124dd09c5854d0520e15d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ee3a546c3d324d318ca11edd4d9ea519": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7fb8d1a256c74f57b47c692057ba0960",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a7163b9342df463c8b18b1bb69117a68"
          }
        },
        "398dbdc7423a414db0f81e832f48e56a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ba34e62c1f9b4469b28c743bb5ccdf9f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 5000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 5000,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_665f6d76a8e148f8b7bb507fb1845ed3"
          }
        },
        "1069b4b20b9649969f0f47ee32d708c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_974c5b3247b7434da5641685993cb764",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 5000/5000 [02:38&lt;00:00, 32.57it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bd3283343fe249b98589951a0b6411fe"
          }
        },
        "7fb8d1a256c74f57b47c692057ba0960": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a7163b9342df463c8b18b1bb69117a68": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ba34e62c1f9b4469b28c743bb5ccdf9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "665f6d76a8e148f8b7bb507fb1845ed3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "974c5b3247b7434da5641685993cb764": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bd3283343fe249b98589951a0b6411fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}