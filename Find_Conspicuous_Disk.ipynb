{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Find_Conspicuous_Disk.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3716c12f4efb465cb02b44722b527be5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e06246d368b148bb98469553545cd221",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a73428c4c7e147eea2a04d2084d7e44d",
              "IPY_MODEL_84f7abe0fcd84614959d2f85939ee5db",
              "IPY_MODEL_835c2a53480c4f7688ff275636c032a3"
            ]
          }
        },
        "e06246d368b148bb98469553545cd221": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a73428c4c7e147eea2a04d2084d7e44d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_52f35fd36f66490a82354150d076296f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_11ba154fdc9d485591583c9a3adb9422"
          }
        },
        "84f7abe0fcd84614959d2f85939ee5db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_de444783f7cc4d0d99954c93810819aa",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 5000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 5000,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0d6f6789114f45edb953cfc7e1891ac9"
          }
        },
        "835c2a53480c4f7688ff275636c032a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6bd5e3d6e34f4179bc3a9d450e167f5c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 5000/5000 [02:23&lt;00:00, 35.20it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_05828ff3b11f4ce498a6c760a023d861"
          }
        },
        "52f35fd36f66490a82354150d076296f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "11ba154fdc9d485591583c9a3adb9422": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "de444783f7cc4d0d99954c93810819aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0d6f6789114f45edb953cfc7e1891ac9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6bd5e3d6e34f4179bc3a9d450e167f5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "05828ff3b11f4ce498a6c760a023d861": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Parameters and imports"
      ],
      "metadata": {
        "id": "i5CjJNUz-2Tm"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDmnBDIK6wP9"
      },
      "source": [
        "random_seed = 20211216\n",
        "\n",
        "# Use the \"mini\" training set with 9 images.\n",
        "# fcd_ts_dir = '/content/drive/My Drive/PredatorEye/mini_training_set/'\n",
        "# Use the \"real\" training set with 2008 images\n",
        "fcd_ts_dir = '/content/drive/My Drive/PredatorEye/fcd_training_set/'\n",
        "\n",
        "# max_input_images = 100\n",
        "max_input_images = 'all'\n",
        "\n",
        "# For each image read from dataset \"amplify\" the set by up to 7 additional\n",
        "# variations of the image via rotations and mirroring.\n",
        "# When I tried using 8 I would run out of memory when the training began.\n",
        "# amplification = 1\n",
        "amplification = 6\n",
        "# amplification = 7\n",
        "\n",
        "# Maybe read from image file?\n",
        "# Maybe assert all images are square and this size?\n",
        "fcd_image_size = 1024\n",
        "\n",
        "# Disk diameter, relative to full sized megapixel image.\n",
        "fcd_disk_size = 201\n",
        "\n",
        "# For scaling down the input image size.\n",
        "# input_scale = 1\n",
        "input_scale = 0.125\n",
        "# input_scale = 0.25\n",
        "# input_scale = 0.5\n",
        "if (input_scale != 1):\n",
        "    fcd_image_size = int(fcd_image_size * input_scale)\n",
        "    # does this really want to be an int?\n",
        "    fcd_disk_size = int(fcd_disk_size * input_scale)\n",
        "\n",
        "fcd_epochs = 100\n",
        "# fcd_epochs = 40\n",
        "fcd_batch_size = 32"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-QjNIB8oGDm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7962e424-1e01-4ba9-80d2-a5d615ee02f8"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "print('TensorFlow version:', tf.__version__)\n",
        "\n",
        "import gc\n",
        "import PIL\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from os import listdir\n",
        "from os.path import join\n",
        "from tqdm.auto import tqdm\n",
        "from matplotlib import image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# from DLAVA, includes unused symbols, maybe tighten later\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.constraints import MaxNorm\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from tensorflow.keras.losses import Loss\n",
        "\n",
        "## maybe just write these inline in the code below?\n",
        "from numpy import asarray \n",
        "from tensorflow.keras import backend as keras_backend\n",
        "keras_backend.set_image_data_format('channels_last')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDFOLYNF0oSj",
        "outputId": "0748b409-2ec1-4ec0-deb0-90f75afc3948"
      },
      "source": [
        "# Check for Colab Pro resources\n",
        "def check_colab_resources():\n",
        "    gpu_info = !nvidia-smi\n",
        "    gpu_info = '\\n'.join(gpu_info)\n",
        "    if gpu_info.find('failed') >= 0:\n",
        "        print('Not connected to a GPU')\n",
        "    else:\n",
        "        print(gpu_info)\n",
        "    from psutil import virtual_memory\n",
        "    ram_gb = virtual_memory().total / 1e9\n",
        "    if ram_gb < 20:\n",
        "        print('Not using a high-RAM runtime.')\n",
        "    else:\n",
        "        print('Using a high-RAM runtime.')\n",
        "    print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "check_colab_resources()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Dec 18 01:07:43 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   48C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "Using a high-RAM runtime.\n",
            "Your runtime has 27.3 gigabytes of available RAM\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utilities"
      ],
      "metadata": {
        "id": "ZCQFpDu-B0HU"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EhCRj-wGwpE"
      },
      "source": [
        "# Prints \"expression = <value>\"\n",
        "def debug_print(expression):\n",
        "    print(expression, '=', eval(expression))\n",
        "\n",
        "# Reset random sequence seeds in Python's \"random\", Numpy, and TensorFlow.\n",
        "def reset_random_seeds():\n",
        "    random.seed(random_seed)\n",
        "    np.random.seed(random_seed)\n",
        "    tf.random.set_seed(random_seed)\n",
        "\n",
        "# Parse FCD filename to a list of two ints: (x, y) pixel coordinates.\n",
        "def fcd_filename_to_xy_ints(filename):\n",
        "    without_extension = filename.split('.')[0]\n",
        "    two_numeric_strings = without_extension.split('_')[1:3]\n",
        "    return list(map(int, two_numeric_strings))\n",
        "\n",
        "# Get image label from image file names ([x, y] as floats on [0,1])\n",
        "def fcd_normalized_xy(filename, pixels):\n",
        "    pixel_coordinates = fcd_filename_to_xy_ints(filename)\n",
        "    return pixel_coordinates / (np.array(pixels.shape)[1:2] / input_scale)\n",
        "\n",
        "# Draw a training image on the log. First arg is either a 24 bit RGB pixel\n",
        "# representation as read from file, or the rescaled 3xfloat used internally.\n",
        "# Optionally draw crosshairs to show center of disk.\n",
        "def draw_image(rgb_pixel_tensor, center=(0,0)):\n",
        "    i24bit = []\n",
        "    if (rgb_pixel_tensor.dtype == np.double):\n",
        "        unscaled_pixels = np.interp(rgb_pixel_tensor, [0, 1], [0, 255])\n",
        "        i24bit = Image.fromarray(unscaled_pixels.astype('uint8'), mode='RGB')\n",
        "    else:\n",
        "        i24bit = Image.fromarray(rgb_pixel_tensor)\n",
        "    plt.imshow(i24bit)\n",
        "    if (center != (0,0)):\n",
        "        draw_crosshairs(center)\n",
        "    plt.show()\n",
        "\n",
        "# Draw crosshairs to indicate disk position (label or estimate).\n",
        "def draw_crosshairs(center):\n",
        "    m = fcd_image_size - 1       # max image coordinate\n",
        "    s = fcd_disk_size * 1.2 / 2  # gap size (radius)\n",
        "    h = center[0] * m            # center x in pixels\n",
        "    v = center[1] * m            # center y in pixels\n",
        "    plt.hlines(v, 0, max(0, h - s), color=\"black\")\n",
        "    plt.hlines(v, min(m, h + s), m, color=\"black\")\n",
        "    plt.vlines(h, 0, max(0, v - s), color=\"white\")\n",
        "    plt.vlines(h, min(m, v + s), m, color=\"white\")\n",
        "\n",
        "# Draw line in plot between arbitrary points in plot.\n",
        "# eg: draw_line((100, 100), (924, 924), color=\"yellow\")\n",
        "def draw_line(p1, p2, color=\"white\"):\n",
        "    plt.plot([p1[0], p2[0]], [p1[1], p2[1]], color)\n",
        "\n",
        "# debug_print('fcd_filename_to_xy_ints(\"foobar_123_456\")')\n",
        "# debug_print('fcd_normalized_xy(\"foobar_123_456\", np.zeros((1024,1024,3)))')\n",
        "# debug_print('[123/(1024/input_scale), 456/(1024/input_scale)]')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training data reader"
      ],
      "metadata": {
        "id": "uyhUYHixCheD"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5Bdh3inPGRM"
      },
      "source": [
        "# Loads FCD training data image files from \"directory_pathname\". Returns an\n",
        "# array of images and an array of labels (each an XY pair, the relative position\n",
        "# of the disk center). Optional \"image_count\" can limit the number of images\n",
        "# read, by taking a random sample of availble image files, defaults to \"all\".\n",
        "\n",
        "def read_fcd_data_from_directory(directory_pathname, image_count = 'all'):\n",
        "    directory_contents = listdir(directory_pathname)\n",
        "    if (image_count == 'all'): image_count = len(directory_contents)\n",
        "    assert image_count <= len(directory_contents), \"Too few images in directory\"\n",
        "    directory_contents = random.sample(directory_contents, image_count)\n",
        "    image_count *= amplification # for rot/mir\n",
        "    print('Reading', max_input_images, 'images from ' + fcd_ts_dir)\n",
        "    print('With an amplification factor of', amplification,\n",
        "          'for a total of', image_count, 'images in dataset.')\n",
        "\n",
        "    # Pre-allocate a tensor for all image data and one for all labels.\n",
        "    local_images = np.zeros([image_count, fcd_image_size, fcd_image_size, 3])\n",
        "    local_labels = np.zeros([image_count, 2])\n",
        "    image_index = 0\n",
        "    for filename in tqdm(directory_contents):\n",
        "        image_pathname = join(directory_pathname, filename)\n",
        "        # Numpy pixel array of image object.\n",
        "        image = Image.open(image_pathname)\n",
        "        new_size = (fcd_image_size, fcd_image_size)\n",
        "        pixels = asarray(image.resize(new_size, PIL.Image.LANCZOS))\n",
        "        # Convert input image data to floating-point.\n",
        "        float_pixels = keras_backend.cast_to_floatx(pixels)\n",
        "        # Scale input image data to range [0, 1] (in DLAVA was [-1, 1])\n",
        "        scaled_pixels = np.interp(float_pixels, [0, 255], [0, 1])\n",
        "        # Read disk center position from file name.\n",
        "        center_position = fcd_normalized_xy(filename, pixels)\n",
        "        def center_rot90(cp): return (cp[1], 0.5 - (cp[0] - 0.5))\n",
        "        def center_flip(cp): return (0.5 - (cp[0] - 0.5), cp[1])\n",
        "        variations = 8  # 4 from rotations times two from mirroring\n",
        "        assert ((amplification > 0) and (amplification <= variations))\n",
        "        keepers = random.sample(range(1, variations), amplification - 1)\n",
        "        keepers.append(0)\n",
        "        for i in range(variations):\n",
        "            if (keepers.count(i) > 0):\n",
        "                # Copy pixel data into slice \"image_index\" of \"local_images\"\n",
        "                local_images[image_index, :, :, :] = scaled_pixels\n",
        "                # Copy disk center XY position into slice of \"local_labels\".\n",
        "                local_labels[image_index, :] = center_position\n",
        "                image_index += 1\n",
        "                draw_frequency = 50 * amplification\n",
        "                # 20211216 is this using up too much memory (for amp=8)?\n",
        "                # if ((image_index % draw_frequency) == draw_frequency - 1):\n",
        "                #     draw_image(scaled_pixels, center_position)\n",
        "                #     print(image_index + 1, \"of\", image_count, \"images...\")\n",
        "            if (i < 7):\n",
        "                if (i == 3):\n",
        "                    scaled_pixels = np.flip(scaled_pixels, axis=1)\n",
        "                    center_position = center_flip(center_position)\n",
        "                else:\n",
        "                    scaled_pixels = np.rot90(scaled_pixels, k=1, axes=(0, 1))\n",
        "                    center_position = center_rot90(center_position)\n",
        "    return local_images, local_labels"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Distance-based loss"
      ],
      "metadata": {
        "id": "JBZiONqEC7Mq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class FCDPositionLoss(Loss):\n",
        "#     # def __init__(self, regularization_factor=0.1, name=\"custom_mse\"):\n",
        "#     def __init__(self,\n",
        "#                  # diameter = fcd_image_size / fcd_disk_size,\n",
        "#                  diameter = float(fcd_disk_size) / float(fcd_image_size),\n",
        "#                  name = \"custom_mse\"):\n",
        "#         super().__init__(name=\"fcd_position_loss\")\n",
        "#         self.diameter = diameter\n",
        "\n",
        "#     def call(self, y_true, y_pred):\n",
        "#         # mse = tf.math.reduce_mean(tf.square(y_true - y_pred))\n",
        "#         # reg = tf.math.reduce_mean(tf.square(0.5 - y_pred))\n",
        "#         # return mse + reg * self.regularization_factor\n",
        "#         # return fcd_position_loss_helper(y_true, y_pred)\n",
        "#         return corresponding_distances(y_true, y_pred)\n",
        "\n",
        "# class FCDDiskShapedLoss(Loss):\n",
        "#     # def __init__(self, regularization_factor=0.1, name=\"custom_mse\"):\n",
        "#     def __init__(self,\n",
        "#                  # diameter = fcd_image_size / fcd_disk_size,\n",
        "#                 #  diameter = float(fcd_disk_size) / float(fcd_image_size),\n",
        "#                  radius = (float(fcd_disk_size) / float(fcd_image_size)) / 2,\n",
        "#                  name = \"custom_mse\"):\n",
        "#         super().__init__(name=\"fcd_disk_shaped_loss\")\n",
        "#         self.radius = radius\n",
        "\n",
        "#     def call(self, y_true, y_pred):\n",
        "#         # d = corresponding_distances(y_true, y_pred)\n",
        "#         # print(\"d\", d)\n",
        "#         # scaled = (d / self.radius)\n",
        "#         # print(\"scaled\", scaled)\n",
        "#         # exponentiated = scaled ** 4\n",
        "#         # print(\"exponentiated\", exponentiated)\n",
        "#         # return exponentiated\n",
        "#         # return fcd_disk_shaped_loss_helper(self.radius, y_true, y_pred)\n",
        "#         return fcd_disk_shaped_loss_helper(y_true, y_pred, self.radius)\n",
        "\n",
        "# Calculates RELATIVE disk radius on the fly -- rewrite later.\n",
        "def fcd_disk_radius():\n",
        "    return (float(fcd_disk_size) / float(fcd_image_size)) / 2\n",
        "\n",
        "def fcd_disk_shaped_loss_helper(y_true, y_pred):\n",
        "    radius = fcd_disk_radius()\n",
        "    d = corresponding_distances(y_true, y_pred)\n",
        "    # print(\"d\", d)\n",
        "    scaled = d / radius\n",
        "    # print(\"scaled\", scaled)\n",
        "    exponentiated = scaled ** 4\n",
        "    # print(\"exponentiated\", exponentiated)\n",
        "    return exponentiated\n",
        "\n",
        "# Given two tensors of 2d point coordinates, return a tensor of the Cartesian\n",
        "# distance between corresponding points in the input tensors.\n",
        "def corresponding_distances(y_true, y_pred):\n",
        "    true_pos_x, true_pos_y = tf.split(y_true, num_or_size_splits=2, axis=1)\n",
        "    pred_pos_x, pred_pos_y = tf.split(y_pred, num_or_size_splits=2, axis=1)\n",
        "    dx = true_pos_x - pred_pos_x\n",
        "    dy = true_pos_y - pred_pos_y\n",
        "    distances = tf.sqrt(tf.square(dx) + tf.square(dy))\n",
        "    return distances"
      ],
      "metadata": {
        "id": "tGGenXdUprZY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prototype metric to measure the fraction of predictions that are inside disks.\n",
        "# For each pair of 2d points of input, output tensor is 1 for IN and 0 for OUT.\n",
        "# def fcd_prediction_inside_disk(y_true, y_pred):\n",
        "\n",
        "# (make name shorter so it is easier to read fit() log.)\n",
        "def in_disk(y_true, y_pred):\n",
        "    distances = corresponding_distances(y_true, y_pred)\n",
        "    # relative_disk_radius = (float(fcd_disk_size) / float(fcd_image_size)) / 2\n",
        "\n",
        "    # From https://stackoverflow.com/a/42450565/1991373\n",
        "    # Boolean tensor marking where distances are less than relative_disk_radius.\n",
        "    # insides = tf.less(distances, relative_disk_radius)\n",
        "    insides = tf.less(distances, fcd_disk_radius())\n",
        "    map_to_zero_or_one = tf.cast(insides, tf.int32)\n",
        "    return map_to_zero_or_one\n",
        "\n",
        "\n",
        "example_true_positions = tf.convert_to_tensor([[1.0, 2.0],\n",
        "                                               [3.0, 4.0],\n",
        "                                               [5.0, 6.0],\n",
        "                                               [7.0, 8.0]])\n",
        "example_pred_positions = tf.convert_to_tensor([[1.1, 2.0],\n",
        "                                               [3.0, 4.2],\n",
        "                                            #    [5.0, 6.1],\n",
        "                                               [5.0, 6.0],\n",
        "                                               [7.3, 8.0]])\n",
        "\n",
        "in_disk(example_true_positions, example_pred_positions)\n",
        "fcd_disk_shaped_loss_helper(example_true_positions, example_pred_positions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-ljSOcNNU31",
        "outputId": "9ed43d2d-4d09-4b88-d687-315e56b9d71f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(4, 1), dtype=float32, numpy=\n",
              "array([[ 1.0995128],\n",
              "       [17.592115 ],\n",
              "       [ 0.       ],\n",
              "       [89.06067  ]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Keras model utilities"
      ],
      "metadata": {
        "id": "HgVOfEMvGF30"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0rTXiq-3su_"
      },
      "source": [
        "# Construct a Keras model with CNN layers at the front, striding down in\n",
        "# resolution, then dense layers funneling down to just two output neurons\n",
        "# representing the predicted image position center of the conspicuous disk.\n",
        "# (First version cribbed from DLAVA chapter B3, Listing B3-41)\n",
        "\n",
        "# def make_striding_cnn_model():\n",
        "def make_fcd_cnn_model():\n",
        "    cnn_act = 'relu'\n",
        "    dense_act = 'relu'\n",
        "    output_act = 'linear'\n",
        "    cnn_dropout = 0.2\n",
        "    dense_dropout = 0.5  # ala Hinton (2012)\n",
        "    model = Sequential()\n",
        "    # Two units of:\n",
        "    #     2 CNN layers with 3x3 32 filters, second one striding down by half\n",
        "    #     followed by dropout\n",
        "    #\n",
        "    # 20211215 maybe use 5x5 for first of each layer? more drop out?\n",
        "    # 20211217 change 3x3/3x3 to 5x5/3x3, now try 7x7/5x5\n",
        "    #          temporarily turn off dropout\n",
        "    #          temporarily switch back to MSE\n",
        "    #          go back to 5x5/3x3\n",
        "    #          add back all dropout\n",
        "    #          add dropout to next 2 dense layers (was 512, now add 128 and 32)\n",
        "    #          nope, remove that.\n",
        "    #          try doubling the number of filters in the CNN layer 32 to 64.\n",
        "    #          nope, remove that.\n",
        "    #          In the 2 CNN groups, add a second 3x3 CNN/dropout between other 2\n",
        "\n",
        "    # model.add(Conv2D(32, (3, 3), activation=cnn_act, padding='same',\n",
        "    model.add(Conv2D(32, (5, 5), activation=cnn_act, padding='same',\n",
        "    # model.add(Conv2D(64, (5, 5), activation=cnn_act, padding='same',\n",
        "    # model.add(Conv2D(32, (7, 7), activation=cnn_act, padding='same',\n",
        "                     kernel_constraint=MaxNorm(3),\n",
        "                     input_shape=(fcd_image_size, fcd_image_size, 3)))\n",
        "    # model.add(Dropout(cnn_dropout)) # added 20211215 9:30 am ###################\n",
        "    model.add(Dropout(cnn_dropout))\n",
        "\n",
        "    # 20211217 4:45pm add a second 3x3 CNN/dropout between other 2:\n",
        "    model.add(Conv2D(32, (3, 3), activation=cnn_act, padding='same',\n",
        "                     kernel_constraint=MaxNorm(3)))\n",
        "    model.add(Dropout(cnn_dropout))\n",
        "\n",
        "    model.add(Conv2D(32, (3, 3), activation=cnn_act, padding='same',\n",
        "    # model.add(Conv2D(64, (3, 3), activation=cnn_act, padding='same',\n",
        "    # model.add(Conv2D(32, (5, 5), activation=cnn_act, padding='same',\n",
        "                     strides=(2, 2), kernel_constraint=MaxNorm(3)))\n",
        "    # model.add(Dropout(cnn_dropout)) # removed 20211215 9:55 am ###############\n",
        "    model.add(Dropout(cnn_dropout))\n",
        "\n",
        "    # model.add(Conv2D(32, (3, 3), activation=cnn_act, padding='same', \n",
        "    model.add(Conv2D(32, (5, 5), activation=cnn_act, padding='same', \n",
        "    # model.add(Conv2D(64, (5, 5), activation=cnn_act, padding='same', \n",
        "    # model.add(Conv2D(32, (7, 7), activation=cnn_act, padding='same', \n",
        "                     kernel_constraint=MaxNorm(3)))\n",
        "    # model.add(Dropout(cnn_dropout)) # added 20211215 9:30 am ###################\n",
        "    model.add(Dropout(cnn_dropout))\n",
        "\n",
        "    # 20211217 4:45pm add a second 3x3 CNN/dropout between other 2:\n",
        "    model.add(Conv2D(32, (3, 3), activation=cnn_act, padding='same',\n",
        "                     kernel_constraint=MaxNorm(3)))\n",
        "    model.add(Dropout(cnn_dropout))\n",
        "\n",
        "    model.add(Conv2D(32, (3, 3), activation=cnn_act, padding='same', \n",
        "    # model.add(Conv2D(64, (3, 3), activation=cnn_act, padding='same', \n",
        "    # model.add(Conv2D(32, (5, 5), activation=cnn_act, padding='same', \n",
        "                     strides=(2, 2), kernel_constraint=MaxNorm(3)))\n",
        "    # model.add(Dropout(cnn_dropout)) # removed 20211215 9:55 am ###############\n",
        "    model.add(Dropout(cnn_dropout))\n",
        "\n",
        "    # 20211215 double largest size, decrease by factor of 4, etc.\n",
        "    # try again at 10:50am, with dropout of 0.3 instead of 0.5\n",
        "\n",
        "    # Then flatten and use a large-ish dense layer with heavy dropout.\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(512, activation=dense_act))\n",
        "    \n",
        "    # model.add(Dropout(dense_dropout))\n",
        "    # model.add(Dropout(0.3))\n",
        "    model.add(Dropout(dense_dropout))\n",
        "\n",
        "    # Then funnel down to two output neurons for (x, y) of predicted center.\n",
        "    model.add(Dense(128, activation=dense_act))\n",
        "    # model.add(Dropout(dense_dropout))\n",
        "    model.add(Dense(32, activation=dense_act))\n",
        "    # model.add(Dropout(dense_dropout))\n",
        "    model.add(Dense(8, activation=dense_act))\n",
        "    model.add(Dense(2, activation=output_act))\n",
        "\n",
        "    # Compile with disk-shaped loss, tracking accuracy and fraction-inside-disk.\n",
        "    # model.compile(loss=fcd_disk_shaped_loss_helper,\n",
        "    model.compile(loss='mse',\n",
        "                  optimizer='adam', metrics=[\"accuracy\", in_disk])\n",
        "    return model"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6A5Obp7DjWr"
      },
      "source": [
        "# Utility to fit and plot a run, again cribbed from DLAVA chapter B3.\n",
        "def run_model(model_maker, plot_title):\n",
        "    model = model_maker()\n",
        "    print(\"In run_model():\")\n",
        "    debug_print('X_train.shape')\n",
        "    debug_print(\"y_train.shape\")\n",
        "    history = model.fit(X_train, y_train, validation_split=0.2,\n",
        "                        epochs=fcd_epochs, batch_size=fcd_batch_size)\n",
        "    print()\n",
        "    plot_accuracy_and_loss(history, plot_title)\n",
        "    return model, history\n",
        "\n",
        "# A little utility to draw plots of accuracy and loss.\n",
        "def plot_accuracy_and_loss(history, plot_title):\n",
        "    xs = range(len(history.history['accuracy']))\n",
        "    # plt.figure(figsize=(10,3))\n",
        "    plt.figure(figsize=(15,3))\n",
        "\n",
        "    # plt.subplot(1, 2, 1)\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.plot(xs, history.history['accuracy'], label='train')\n",
        "    plt.plot(xs, history.history['val_accuracy'], label='validation')\n",
        "    plt.legend(loc='lower left')\n",
        "    plt.xlabel('epochs')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.title(plot_title+': Accuracy')\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    # plt.plot(xs, history.history['fcd_prediction_inside_disk'], label='train')\n",
        "    # plt.plot(xs, history.history['val_fcd_prediction_inside_disk'], label='validation')\n",
        "    plt.plot(xs, history.history['in_disk'], label='train')\n",
        "    plt.plot(xs, history.history['val_in_disk'], label='validation')\n",
        "    plt.legend(loc='lower left')\n",
        "    plt.xlabel('epochs')\n",
        "    plt.ylabel('fraction inside disk')\n",
        "    plt.title(plot_title+': fraction inside disk')\n",
        "\n",
        "    # plt.subplot(1, 2, 2)\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.plot(xs, history.history['loss'], label='train')\n",
        "    plt.plot(xs, history.history['val_loss'], label='validation')\n",
        "    plt.legend(loc='upper left')\n",
        "    plt.xlabel('epochs')\n",
        "    plt.ylabel('loss')\n",
        "    plt.title(plot_title+': Loss')\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read training data"
      ],
      "metadata": {
        "id": "9Iot7WB8KhGg"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "3716c12f4efb465cb02b44722b527be5",
            "e06246d368b148bb98469553545cd221",
            "a73428c4c7e147eea2a04d2084d7e44d",
            "84f7abe0fcd84614959d2f85939ee5db",
            "835c2a53480c4f7688ff275636c032a3",
            "52f35fd36f66490a82354150d076296f",
            "11ba154fdc9d485591583c9a3adb9422",
            "de444783f7cc4d0d99954c93810819aa",
            "0d6f6789114f45edb953cfc7e1891ac9",
            "6bd5e3d6e34f4179bc3a9d450e167f5c",
            "05828ff3b11f4ce498a6c760a023d861"
          ]
        },
        "id": "zKUhDLbgJxtR",
        "outputId": "6399324b-054f-4a35-f892-b14d65796b20"
      },
      "source": [
        "# Read FCD training data from a given directory.\n",
        "reset_random_seeds()\n",
        "start_time = time.time()\n",
        "(X_train, y_train) = ([], [])  # To release memory when rerunning in notebook.\n",
        "gc.collect()\n",
        "(X_train, y_train) = read_fcd_data_from_directory(fcd_ts_dir, max_input_images)\n",
        "print('Total of', X_train.shape[0], 'labeled images.')\n",
        "print('Elapsed time:', int(time.time() - start_time), 'seconds.')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading all images from /content/drive/My Drive/PredatorEye/fcd_training_set/\n",
            "With an amplification factor of 6 for a total of 30000 images in dataset.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3716c12f4efb465cb02b44722b527be5",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/5000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total of 30000 labeled images.\n",
            "Elapsed time: 149 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build and train model"
      ],
      "metadata": {
        "id": "CQy5wa_GLNG4"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ppYkZ-grG2E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00b8745a-7a38-456b-a475-a0fd7141cc21"
      },
      "source": [
        "# Run a model.\n",
        "reset_random_seeds()\n",
        "start_time = time.time()\n",
        "(model, history) = ([], [])  # To release memory when rerunning in notebook.\n",
        "gc.collect()\n",
        "# (model, history) = run_model(make_striding_cnn_model, 'FCD')\n",
        "(model, history) = run_model(make_fcd_cnn_model, 'FCD')\n",
        "print('Elapsed time:', int(time.time() - start_time), 'seconds.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In run_model():\n",
            "X_train.shape = (30000, 128, 128, 3)\n",
            "y_train.shape = (30000, 2)\n",
            "Epoch 1/100\n",
            "750/750 [==============================] - 37s 37ms/step - loss: 0.0603 - accuracy: 0.5041 - in_disk: 0.0468 - val_loss: 0.0569 - val_accuracy: 0.4993 - val_in_disk: 0.0492\n",
            "Epoch 2/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 0.0556 - accuracy: 0.4970 - in_disk: 0.0495 - val_loss: 0.0559 - val_accuracy: 0.4993 - val_in_disk: 0.0508\n",
            "Epoch 3/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 0.0547 - accuracy: 0.4960 - in_disk: 0.0530 - val_loss: 0.0524 - val_accuracy: 0.4990 - val_in_disk: 0.0627\n",
            "Epoch 4/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 0.0449 - accuracy: 0.5061 - in_disk: 0.0998 - val_loss: 0.0432 - val_accuracy: 0.4998 - val_in_disk: 0.1378\n",
            "Epoch 5/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 0.0399 - accuracy: 0.5020 - in_disk: 0.1399 - val_loss: 0.0392 - val_accuracy: 0.4993 - val_in_disk: 0.1630\n",
            "Epoch 6/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 0.0273 - accuracy: 0.7364 - in_disk: 0.3404 - val_loss: 0.0212 - val_accuracy: 0.8197 - val_in_disk: 0.5585\n",
            "Epoch 7/100\n",
            "749/750 [============================>.] - ETA: 0s - loss: 0.0194 - accuracy: 0.8242 - in_disk: 0.5184"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analyze results"
      ],
      "metadata": {
        "id": "gYrWnkXtLVcO"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfqgguszL0sv"
      },
      "source": [
        "# Draw some results to understand performance\n",
        "# TODO needs to discriminate between training and validation sets.\n",
        "#      feature best/worse results of both cases?\n",
        "# 20211210 refactor to not run predict on whole training set\n",
        "def draw_results(count = 20):\n",
        "    for i in random.sample(range(X_train.shape[0]), count) :\n",
        "        pixel_tensor = X_train[i, :, :, :]\n",
        "        predict = model.predict(tf.convert_to_tensor([pixel_tensor]))[0]\n",
        "        x = predict[0]\n",
        "        y = predict[1]\n",
        "        print(i, \": (\", x, \",\", y, \")\")\n",
        "        draw_image(X_train[i, :, :, :], [x, y])\n",
        "\n",
        "reset_random_seeds()\n",
        "draw_results()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}