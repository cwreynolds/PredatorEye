{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5CjJNUz-2Tm"
      },
      "source": [
        "# Parameters and imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDmnBDIK6wP9"
      },
      "outputs": [],
      "source": [
        "# random_seed = 20211216\n",
        "# random_seed = 20211218\n",
        "# random_seed = 20211221\n",
        "random_seed = 20211222\n",
        "\n",
        "# Use the \"mini\" training set with 9 images.\n",
        "# fcd_ts_dir = '/content/drive/My Drive/PredatorEye/mini_training_set/'\n",
        "# Use the \"real\" training set with 2008 images\n",
        "fcd_ts_dir = '/content/drive/My Drive/PredatorEye/fcd_training_set/'\n",
        "\n",
        "model_save_directory = '/content/drive/My Drive/PredatorEye/saved_models/'\n",
        "\n",
        "max_input_images = 100\n",
        "# max_input_images = 'all'\n",
        "\n",
        "# For each image read from dataset \"amplify\" the set by up to 7 additional\n",
        "# variations of the image via rotations and mirroring.\n",
        "# When I tried using 8 I would run out of memory when the training began.\n",
        "# amplification = 1\n",
        "# amplification = 6\n",
        "amplification = 8\n",
        "\n",
        "# amplification = 16 # add intensity inversion, filled memory\n",
        "# amplification = 12\n",
        "# amplification = 10\n",
        "# amplification = 9\n",
        "\n",
        "# 20211221\n",
        "# Allow amplification by anopther factor of 2 by inverting image brightness.\n",
        "# allow_amp_by_inv = True\n",
        "allow_amp_by_inv = False\n",
        "\n",
        "# Maybe read from image file?\n",
        "# Maybe assert all images are square and this size?\n",
        "fcd_image_size = 1024\n",
        "\n",
        "# Disk diameter, relative to full sized megapixel image.\n",
        "fcd_disk_size = 201\n",
        "\n",
        "# For scaling down the input image size.\n",
        "# input_scale = 1\n",
        "input_scale = 0.125\n",
        "# input_scale = 0.25\n",
        "# input_scale = 0.5\n",
        "if (input_scale != 1):\n",
        "    fcd_image_size = int(fcd_image_size * input_scale)\n",
        "    # does this really want to be an int?\n",
        "    fcd_disk_size = int(fcd_disk_size * input_scale)\n",
        "\n",
        "fcd_epochs = 100\n",
        "# fcd_epochs = 40\n",
        "\n",
        "# On 20211218 increased from 32 to 128 which seemed to help stability.\n",
        "fcd_batch_size = 128\n",
        "# # 20211221\n",
        "# fcd_batch_size = 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-QjNIB8oGDm"
      },
      "outputs": [],
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "print('TensorFlow version:', tf.__version__)\n",
        "\n",
        "import gc\n",
        "import PIL\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from os import listdir\n",
        "from os.path import join\n",
        "from tqdm.auto import tqdm\n",
        "from matplotlib import image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras.losses import Loss\n",
        "\n",
        "## maybe just write these inline in the code below?\n",
        "from numpy import asarray \n",
        "from tensorflow.keras import backend as keras_backend\n",
        "keras_backend.set_image_data_format('channels_last')\n",
        "\n",
        "# 20211218\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Import DiskFind utilities for PredatorEye.\n",
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/PredatorEye/shared_code/')\n",
        "import DiskFind"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDFOLYNF0oSj"
      },
      "outputs": [],
      "source": [
        "# Check for Colab Pro resources\n",
        "def check_colab_resources():\n",
        "    gpu_info = !nvidia-smi\n",
        "    gpu_info = '\\n'.join(gpu_info)\n",
        "    if gpu_info.find('failed') >= 0:\n",
        "        print('Not connected to a GPU')\n",
        "    else:\n",
        "        print(gpu_info)\n",
        "    from psutil import virtual_memory\n",
        "    ram_gb = virtual_memory().total / 1e9\n",
        "    if ram_gb < 20:\n",
        "        print('Not using a high-RAM runtime.')\n",
        "    else:\n",
        "        print('Using a high-RAM runtime.')\n",
        "    print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "check_colab_resources()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCQFpDu-B0HU"
      },
      "source": [
        "# Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8EhCRj-wGwpE"
      },
      "outputs": [],
      "source": [
        "# Prints \"expression = <value>\"\n",
        "def debug_print(expression):\n",
        "    print(expression, '=', eval(expression))\n",
        "\n",
        "# Reset random sequence seeds in Python's \"random\", Numpy, and TensorFlow.\n",
        "def reset_random_seeds():\n",
        "    random.seed(random_seed)\n",
        "    np.random.seed(random_seed)\n",
        "    tf.random.set_seed(random_seed)\n",
        "\n",
        "# Parse FCD filename to a list of two ints: (x, y) pixel coordinates.\n",
        "def fcd_filename_to_xy_ints(filename):\n",
        "    without_extension = filename.split('.')[0]\n",
        "    two_numeric_strings = without_extension.split('_')[1:3]\n",
        "    return list(map(int, two_numeric_strings))\n",
        "\n",
        "# Get image label from image file names ([x, y] as floats on [0,1])\n",
        "def fcd_normalized_xy(filename, pixels):\n",
        "    pixel_coordinates = fcd_filename_to_xy_ints(filename)\n",
        "    return pixel_coordinates / (np.array(pixels.shape)[1:2] / input_scale)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyhUYHixCheD"
      },
      "source": [
        "# Data reader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5Bdh3inPGRM"
      },
      "outputs": [],
      "source": [
        "# Loads FCD training data image files from \"directory_pathname\". Returns an\n",
        "# array of images and an array of labels (each an XY pair, the relative position\n",
        "# of the disk center). Optional \"image_count\" can limit the number of images\n",
        "# read, by taking a random sample of availble image files, defaults to \"all\".\n",
        "\n",
        "def read_fcd_data_from_directory(directory_pathname, image_count = 'all'):\n",
        "    directory_contents = listdir(directory_pathname)\n",
        "    if (image_count == 'all'): image_count = len(directory_contents)\n",
        "    assert image_count <= len(directory_contents), \"Too few images in directory\"\n",
        "    directory_contents = random.sample(directory_contents, image_count)\n",
        "    image_count *= amplification # for rot/mir\n",
        "    print('Reading', max_input_images, 'images from ' + fcd_ts_dir)\n",
        "    print('With an amplification factor of', amplification,\n",
        "          'for a total of', image_count, 'images in dataset.')\n",
        "    image_list = []\n",
        "    label_list = []\n",
        "    for filename in tqdm(directory_contents):\n",
        "        image_pathname = join(directory_pathname, filename)\n",
        "        # Numpy pixel array of image object.\n",
        "        image = Image.open(image_pathname)\n",
        "        new_size = (fcd_image_size, fcd_image_size)\n",
        "        pixels = asarray(image.resize(new_size, PIL.Image.LANCZOS),\n",
        "                         dtype=np.float32)        \n",
        "        # Convert input image data to floating-point.\n",
        "        float_pixels = keras_backend.cast_to_floatx(pixels)\n",
        "        # Scale input image data to range [0, 1] (in DLAVA was [-1, 1])\n",
        "        scaled_pixels = np.interp(float_pixels, [0, 255], [0, 1])\n",
        "        # Convert float pixel data to 32 bit.\n",
        "        scaled_pixels = scaled_pixels.astype(np.float32)\n",
        "        # Read disk center position from file name.\n",
        "        center_position = fcd_normalized_xy(filename, pixels)\n",
        "        def center_rot90(cp): return (cp[1], 0.5 - (cp[0] - 0.5))\n",
        "        def center_flip(cp): return (0.5 - (cp[0] - 0.5), cp[1])\n",
        "        variations = 8  # 4 from rotations times two from mirroring\n",
        "        if (allow_amp_by_inv):\n",
        "            variations *= 2  # times two for invert intensity \n",
        "        assert ((amplification > 0) and (amplification <= variations))\n",
        "        keepers = random.sample(range(1, variations), amplification - 1)\n",
        "        keepers.append(0)\n",
        "        for i in range(variations):\n",
        "            scaled_pixels = scaled_pixels.astype(np.float32)\n",
        "            if (keepers.count(i) > 0):\n",
        "                # 20220104\n",
        "                # DiskFind.draw_image(scaled_pixels, center_position)\n",
        "                # Append pixel data to \"image_list\"\n",
        "                image_list.append(scaled_pixels)\n",
        "                # Append disk center XY position to \"label_list\".\n",
        "                label_list.append(center_position)\n",
        "            if (i < (variations - 1)):\n",
        "                # better to express this as modulo arithmetic?\n",
        "                if (i == 7):\n",
        "                    scaled_pixels = 1 - scaled_pixels\n",
        "                else:\n",
        "                # better to express this as modulo arithmetic?\n",
        "                    if ((i == 3) or (i == 3 + 8)):\n",
        "                        scaled_pixels = np.flip(scaled_pixels, axis=1)\n",
        "                        center_position = center_flip(center_position)\n",
        "                    else:\n",
        "                        scaled_pixels = np.rot90(scaled_pixels, k=1, axes=(0, 1))\n",
        "                        center_position = center_rot90(center_position)\n",
        "    return image_list, label_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Iot7WB8KhGg"
      },
      "source": [
        "# Read training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKUhDLbgJxtR"
      },
      "outputs": [],
      "source": [
        "# Read FCD training data from a given directory.\n",
        "reset_random_seeds()\n",
        "start_time = time.time()\n",
        "# (X_train, y_train) = ([], [])  # To release memory when rerunning in notebook.\n",
        "# To release memory when rerunning in notebook.\n",
        "(X_train, X_test, y_train, y_test) = ([], [], [], [])  \n",
        "gc.collect()\n",
        "(X_train, y_train) = read_fcd_data_from_directory(fcd_ts_dir, max_input_images)\n",
        "# print('Total of', X_train.shape[0], 'labeled images.')\n",
        "print('Total of', len(X_train), 'labeled images.')\n",
        "\n",
        "# 20211218\n",
        "(X_train, X_test, y_train, y_test) = train_test_split(X_train, y_train,\n",
        "                                                      test_size=0.2)\n",
        "print('len(X_train) =', len(X_train))\n",
        "print('len(y_train) =', len(y_train))\n",
        "print('len(X_test) =', len(X_test))\n",
        "print('len(y_test) =', len(y_test))\n",
        "\n",
        "# Convert from python lists to np array.\n",
        "X_train = np.array(X_train, dtype=np.float32)\n",
        "y_train = np.array(y_train, dtype=np.float32)\n",
        "X_test = np.array(X_test, dtype=np.float32)\n",
        "y_test = np.array(y_test, dtype=np.float32)\n",
        "\n",
        "print('Elapsed time:', int(time.time() - start_time), 'seconds.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQy5wa_GLNG4"
      },
      "source": [
        "# Build and train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ppYkZ-grG2E"
      },
      "outputs": [],
      "source": [
        "# Run a model.\n",
        "reset_random_seeds()\n",
        "start_time = time.time()\n",
        "fcd_model_timestamp = DiskFind.timestamp_string()\n",
        "(model, history) = ([], [])  # To release memory when rerunning in notebook.\n",
        "gc.collect()\n",
        "\n",
        "# (model, history) = run_model(make_fcd_cnn_model, 'FCD')\n",
        "model = DiskFind.make_disk_finder_model(X_train)\n",
        "# history = DiskFind.run_model(model, 'FCD')\n",
        "history = DiskFind.run_model(model, X_train, y_train, X_test, y_test,\n",
        "                             fcd_epochs, fcd_batch_size, 'FCD')\n",
        "\n",
        "elapsed_seconds = int(time.time() - start_time)\n",
        "print('Elapsed time: ' + str(elapsed_seconds) + ' seconds (' +\n",
        "      str(int(elapsed_seconds / 60)) +' minutes).')\n",
        "\n",
        "# model.save(model_save_directory + fcd_model_timestamp)\n",
        "model_save_path = model_save_directory + fcd_model_timestamp\n",
        "model.save(model_save_path)\n",
        "print('Saved trained model to', model_save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYrWnkXtLVcO"
      },
      "source": [
        "# Analyze results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VfqgguszL0sv"
      },
      "outputs": [],
      "source": [
        "# Draw some results to understand performance\n",
        "def draw_results(count = 20):\n",
        "    for i in random.sample(range(X_test.shape[0]), count) :\n",
        "        pixel_tensor = X_test[i, :, :, :]\n",
        "        predict = model.predict(tf.convert_to_tensor([pixel_tensor]))[0]\n",
        "        x = predict[0]\n",
        "        y = predict[1]\n",
        "        print(i, \": (\", x, \",\", y, \")\")\n",
        "        DiskFind.draw_image(pixel_tensor, [x, y])\n",
        "\n",
        "reset_random_seeds()\n",
        "draw_results()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Find_Conspicuous_Disk.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}