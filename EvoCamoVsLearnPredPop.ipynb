{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EvoCamoVsLearnPredPop.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8AbtMQfMUkt"
      },
      "source": [
        "# Evolutionary Camouflage Versus a Learning Predator Population\n",
        "\n",
        "---\n",
        "\n",
        "EvoCamoVsLearnPredPop.ipynb\n",
        "\n",
        "Started as a copy of EvoCamoVsLearningPredator.ipynb as of 20220607\n",
        "\n",
        "On 20220716 starting from here to prototype “local” version running on my Apple Silicon M1 laptop. Got as far as trying to read pre-tained “FCD6_rc4” model but it complained it was compiled for Intel, while this is Apple Silicon.\n",
        "\n",
        "OK now on 20220722 I am converting this back to \"Rube Goldberg\" mode where predator vision happens in the cloud, and camouflage evolution runs on my laptop. I tried using the [TensorFlow-Metal plugin](https://developer.apple.com/metal/tensorflow-plugin/), but ran into a [bug](https://developer.apple.com/forums/thread/706920). Until that is fixed I will fall back to the old half-cloud-half-laptop approach."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Cr9fUxZxJBRl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca21d34a-801e-4d24-dc7e-151d9e7ea0f7"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rube_Goldberg_mode = True\n",
            "shared_directory = /content/drive/My Drive/PredatorEye/evo_camo_vs_static_fcd/\n",
            "saved_model_directory = /content/drive/My Drive/PredatorEye/saved_models/\n",
            "TensorFlow version: 2.8.2\n"
          ]
        }
      ],
      "source": [
        "# \"Rube Goldberg\" mode refers to running camouflage evolution on my laptop while\n",
        "# running predator vision in cloud via Colab. State is passed back and forth via\n",
        "# files on Google Drive.\n",
        "Rube_Goldberg_mode = True\n",
        "def if_RG_mode(for_RG_mode, for_normal_mode):\n",
        "    return for_RG_mode if Rube_Goldberg_mode else for_normal_mode\n",
        "\n",
        "# PredatorEye directory on Drive.\n",
        "pe_directory = '/content/drive/My Drive/PredatorEye/'\n",
        "\n",
        "# Shared \"communication\" directory on Drive.\n",
        "shared_directory = if_RG_mode(pe_directory + 'evo_camo_vs_static_fcd/',\n",
        "                              '/Users/cwr/comms/')\n",
        "\n",
        "# This was meant (20220716) to allow reading original pre-trained model from\n",
        "# Google Drive, but I'll need to retrain it for M1 (Apple Silicon).\n",
        "g_drive_pe_dir = ('/Users/cwr/Library/CloudStorage/' +\n",
        "                  'GoogleDrive-craig.w.reynolds@gmail.com/' +\n",
        "                  'My Drive/PredatorEye/')\n",
        "\n",
        "# Directory for pre-trained Keras/TensorFlow models.\n",
        "saved_model_directory = if_RG_mode(pe_directory, g_drive_pe_dir) + 'saved_models/'\n",
        "\n",
        "\n",
        "print('Rube_Goldberg_mode =', Rube_Goldberg_mode)\n",
        "print('shared_directory =', shared_directory)\n",
        "print('saved_model_directory =', saved_model_directory)\n",
        "\n",
        "# Pathname of pre-trained Keras/TensorFlow model\n",
        "# trained_model = saved_model_directory + '20220202_1211_Find_3_Disks_complex'\n",
        "# trained_model = saved_model_directory + '20220222_1747_F3D_augmented_rc4'\n",
        "# trained_model = saved_model_directory + '20220227_0746_F3D2_a'\n",
        "# trained_model = saved_model_directory + '20220304_1135_FCD5_a'\n",
        "trained_model = saved_model_directory + '20220321_1711_FCD6_rc4'\n",
        "# model = []\n",
        "\n",
        "# Directory on Drive for storing fine-tuning dataset.\n",
        "fine_tuning_directory = shared_directory + 'fine_tuning/'\n",
        "\n",
        "my_prefix = \"find_\"\n",
        "other_prefix = \"camo_\"\n",
        "\n",
        "my_suffix =  \".txt\"\n",
        "# other_suffix = \".jpeg\"\n",
        "other_suffix = \".png\"\n",
        "\n",
        "fcd_image_size = 1024\n",
        "fcd_disk_size = 201\n",
        "\n",
        "import time\n",
        "import PIL\n",
        "from pathlib import Path\n",
        "\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# %tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "print('TensorFlow version:', tf.__version__)\n",
        "\n",
        "from tensorflow.keras import backend as keras_backend\n",
        "keras_backend.set_image_data_format('channels_last')\n",
        "\n",
        "# Import DiskFind utilities for PredatorEye.\n",
        "import sys\n",
        "if Rube_Goldberg_mode:\n",
        "    sys.path.append('/content/drive/My Drive/PredatorEye/shared_code/')\n",
        "else:\n",
        "    sys.path.append('/Users/cwr/Documents/code/PredatorEye/')\n",
        "import DiskFind as df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCxVUKpMTEcN"
      },
      "source": [
        "# Ad hoc “predator server”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZF4XLFSBTKOM"
      },
      "outputs": [],
      "source": [
        "# Top level: wait for camo_xxx.jpeg files to appear, respond with find_xxx.txt\n",
        "def start_run(step = 0):\n",
        "    if step == 0:\n",
        "        print('Start run in', shared_directory )\n",
        "    else:\n",
        "        print('Continue run at step', step, ' in', shared_directory)\n",
        "    while True:\n",
        "        perform_step(step, shared_directory)\n",
        "        step += 1\n",
        "\n",
        "# Continue from from the last camo_xxx.jpeg file.\n",
        "def restart_run():\n",
        "    start_run(newest_file_from_other(shared_directory))\n",
        "\n",
        "# Single step: wait for camo file, write response, delete previous response.\n",
        "def perform_step(step, directory):\n",
        "    wait_for_reply(step, shared_directory)\n",
        "    write_response_file(step, shared_directory)\n",
        "    delete_find_file(step - 1, shared_directory)\n",
        "\n",
        "# Read image file for step, apply pre-trained model, write response file.\n",
        "def write_response_file(step, directory):\n",
        "    # Read image file and check for expected format.\n",
        "    image_pathname = make_camo_pathname(step, directory)\n",
        "    pixel_tensor = df.read_image_file_as_pixel_tensor(image_pathname)\n",
        "    assert df.check_pixel_tensor(pixel_tensor), ('wrong file format: ' +\n",
        "                                                 image_pathname)\n",
        "    \n",
        "    # Default Predator's model -- for prototying.\n",
        "    model = Predator.default_predator().model\n",
        "    # Run Predator's model on new image.\n",
        "    prediction = model.predict(tf.convert_to_tensor([pixel_tensor]))[0]\n",
        "    # Generate response file.\n",
        "    response_string = str(prediction[0]) + \" \" + str(prediction[1])\n",
        "    verify_comms_directory_reachable()\n",
        "    with open(make_find_pathname(step, directory), 'w') as file:\n",
        "        file.write(response_string)\n",
        "    print('Wrote ' + \"'\" + response_string + \"'\",\n",
        "          'to response file', Path(make_find_pathname(step, directory)).name)\n",
        "    # Logging, maybe only for testing, delete me?\n",
        "    sp = Predator.second_predator()\n",
        "    spm_predict = sp.model.predict(tf.convert_to_tensor([pixel_tensor]))[0]\n",
        "    print('\"other\" prediction:', spm_predict,\n",
        "          ' distance to original prediction:', df.dist2d(spm_predict, prediction))\n",
        "\n",
        "    # Merge this step's image into fine-tuning dataset, and related bookkeeping.\n",
        "    fine_tuning_dataset.update(pixel_tensor, prediction, step, directory)\n",
        "\n",
        "    # Predator learns from recent experience.\n",
        "    Predator.default_predator().fine_tune_model(pixel_tensor, prediction,\n",
        "                                                step, directory)\n",
        "    # For testing/prototyping\n",
        "    sp.fine_tune_model(pixel_tensor, prediction, step, directory)\n",
        "\n",
        "# Delete the given file, usually after having written the next one.\n",
        "def delete_find_file(step, directory):\n",
        "    # Why doesn't pathlib provide a Path.remove() method like os?\n",
        "    # TODO oh, missing_ok was added at pathlib version 3.8.\n",
        "    # Path(makeMyPathname(step, directory)).unlink(missing_ok=True)\n",
        "    p = Path(make_find_pathname(step, directory))\n",
        "    if p.exists():\n",
        "        p.unlink()\n",
        "\n",
        "# Delete any remaining file in commuications directory to start a new run.\n",
        "def clean_up_communication_directory():\n",
        "    def delete_directory_contents(directory_path):\n",
        "        for path in directory_path.iterdir():\n",
        "            print('Removing from communication directory:', path)\n",
        "            if path.is_dir():\n",
        "                delete_directory_contents(path)\n",
        "                path.rmdir()\n",
        "            else:\n",
        "                path.unlink()\n",
        "    delete_directory_contents(Path(shared_directory))\n",
        "\n",
        "# From pathname for file of given step number from the \"other\" agent.\n",
        "def make_camo_pathname(step, directory):\n",
        "    return directory + other_prefix + str(step) + other_suffix\n",
        "\n",
        "# Form pathname for \"find_xx.txt\" response file from \"this\" agent.\n",
        "def make_find_pathname(step, directory):\n",
        "    return directory + my_prefix + str(step) + my_suffix\n",
        "\n",
        "# Form pathname for \"prey_xx.txt\" ground truth file from \"other\" agent.\n",
        "def make_prey_pathname(step, directory):\n",
        "    return directory + 'prey_' + str(step) + '.txt'\n",
        "\n",
        "# Used to ping the comms directory when it seems hung.\n",
        "def write_ping_file(count, step, directory):\n",
        "    pn = directory + 'ping_cloud_' + str(step) + '.txt'\n",
        "    verify_comms_directory_reachable()\n",
        "    with open(pn, 'w') as file:\n",
        "        file.write(str(count))\n",
        "    print('Ping comms: ', count, pn)\n",
        "\n",
        "# Wait until other agent's file for given step appears.\n",
        "def wait_for_reply(step, directory):\n",
        "    camo_pathname = Path(make_camo_pathname(step, directory))\n",
        "    camo_filename = camo_pathname.name\n",
        "    prey_pathname = Path(make_prey_pathname(step, directory))\n",
        "    prey_filename = prey_pathname.name\n",
        "    print('Waiting for', camo_filename, 'and', prey_filename, '...',\n",
        "          end='', flush=True)\n",
        "    start_time = time.time()\n",
        "    # Loop until both files are present, waiting 1 second between tests.\n",
        "    test_count = 0\n",
        "    while not (is_file_present(camo_pathname) and\n",
        "               is_file_present(prey_pathname)):\n",
        "        time.sleep(1)\n",
        "        test_count += 1\n",
        "        if (test_count % 100) == 0:\n",
        "            write_ping_file(test_count, step, directory)\n",
        "    print(' done, elapsed time:', int(time.time() - start_time), 'seconds.')\n",
        "\n",
        "# Like fs::exists()\n",
        "def is_file_present(file):\n",
        "    result = False\n",
        "    verify_comms_directory_reachable()\n",
        "    filename = Path(file).name\n",
        "    directory = Path(file).parent\n",
        "    for i in directory.iterdir():\n",
        "        if i.name == filename:\n",
        "            result = True\n",
        "    return result\n",
        "\n",
        "# Returns the step number of the newest file from \"other\" in given directory.\n",
        "# (So if \"camo_573.jpeg\" is the only \"other\" file there, returns int 573)\n",
        "def newest_file_from_other(directory):\n",
        "    steps = [0]  # Default to zero in case dir is empty.\n",
        "    for filename in Path(directory).iterdir():\n",
        "        name = filename.name\n",
        "        if other_prefix == name[0:len(other_prefix)]:\n",
        "            steps.append(int(name.split(\".\")[0].split(\"_\")[1]))\n",
        "    return max(steps)\n",
        "\n",
        "# Read ground truth prey center location data provided in \"prey_n.txt\" file.\n",
        "def read_3_centers_from_file(step, directory):\n",
        "    # Read contents of file as string.\n",
        "    verify_comms_directory_reachable()\n",
        "    with open(make_prey_pathname(step, directory), 'r') as file:\n",
        "        prey_centers_string = file.read()\n",
        "    # Split string at whitespace, map to 6 floats, reshape into 3 xy pairs.\n",
        "    # (TODO could probably be rewritten cleaner with \"list comprehension\")\n",
        "    array = np.reshape(list(map(float, prey_centers_string.split())), (3, 2))\n",
        "    return array.tolist()\n",
        "\n",
        "# Keep log of in_disk metric.\n",
        "def write_in_disk_log(step, history):\n",
        "    if step % 10 == 0:\n",
        "        in_disk = history.history[\"in_disk\"][0]\n",
        "        pathname = shared_directory + 'in_disk_log.csv'\n",
        "        verify_comms_directory_reachable()\n",
        "        with open(pathname, 'a') as file:\n",
        "            if step == 0:\n",
        "                file.write('step,in_disk\\n')\n",
        "            file.write(str(step) + ',' + \"{:.4f}\".format(in_disk) + '\\n')\n",
        "\n",
        "# Just wait in retry loop if shared \"comms\" directory become unreachable.\n",
        "# Probably will return shortly, better to wait than signal a file error.\n",
        "# (This is called from places with a local \"directory\" but it uses global value.)\n",
        "def verify_comms_directory_reachable():\n",
        "    seconds = 0\n",
        "    # shared_directory_pathname = Path(shared_directory)\n",
        "    # while not shared_directory_pathname.is_dir():\n",
        "    while not Path(shared_directory).is_dir():\n",
        "        print(\"Shared “comms” directory,\", shared_directory, \n",
        "              \"has been inaccessible for\", seconds, \"seconds.\")\n",
        "        time.sleep(1)  # wait 1 sec\n",
        "        seconds += 1\n",
        "\n",
        "# Given 3 prey positions (\"xy3\"), sort them by proximity to \"point\" (prediction)\n",
        "def sort_xy3_by_proximity_to_point(xy3, point):\n",
        "    # print('xy3 =', xy3)\n",
        "    xy3_plus_distance = [[df.dist2d(xy, point), xy] for xy in xy3]\n",
        "    # print('xy3_plus_distance =', xy3_plus_distance)\n",
        "    sorted_xy3_plus_key = sorted(xy3_plus_distance, key=lambda x: x[0])\n",
        "    # print('sorted_xy3_plus_key =', sorted_xy3_plus_key)\n",
        "    sorted_xy3 = [x[1] for x in sorted_xy3_plus_key]\n",
        "    # print('sorted_xy3 =', sorted_xy3)\n",
        "    return sorted_xy3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDtbk88sVGxk"
      },
      "source": [
        "# Read pre-trained model\n",
        "\n",
        "As I integrate this into the Predator class, this is no longer “Read pre-trained model” but more like “Some utilities for reading the pre-trained model”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iLVIRi_vU9gN"
      },
      "outputs": [],
      "source": [
        "# Read pre-trained TensorFlow \"predator vision\" model.\n",
        "\n",
        "# print('Reading pre-trained model from:', trained_model)\n",
        "\n",
        "# ad hoc workaround suggested on https://stackoverflow.com/q/66408995/1991373\n",
        "#\n",
        "# dependencies = {\n",
        "#     'hamming_loss': tfa.metrics.HammingLoss(mode=\"multilabel\", name=\"hamming_loss\"),\n",
        "#     'attention': attention(return_sequences=True)\n",
        "# }\n",
        "#\n",
        "# dependencies = {\n",
        "#     'valid_accuracy': ValidAccuracy\n",
        "# }\n",
        "\n",
        "# Calculates RELATIVE disk radius on the fly -- rewrite later.\n",
        "def fcd_disk_radius():\n",
        "    return (float(fcd_disk_size) / float(fcd_image_size)) / 2\n",
        "\n",
        "# Given two tensors of 2d point coordinates, return a tensor of the Cartesian\n",
        "# distance between corresponding points in the input tensors.\n",
        "def corresponding_distances(y_true, y_pred):\n",
        "    true_pos_x, true_pos_y = tf.split(y_true, num_or_size_splits=2, axis=1)\n",
        "    pred_pos_x, pred_pos_y = tf.split(y_pred, num_or_size_splits=2, axis=1)\n",
        "    dx = true_pos_x - pred_pos_x\n",
        "    dy = true_pos_y - pred_pos_y\n",
        "    distances = tf.sqrt(tf.square(dx) + tf.square(dy))\n",
        "    return distances\n",
        "\n",
        "# 20211231 copied from Find_Concpocuous_Disk\n",
        "def in_disk(y_true, y_pred):\n",
        "    distances = corresponding_distances(y_true, y_pred)\n",
        "    # relative_disk_radius = (float(fcd_disk_size) / float(fcd_image_size)) / 2\n",
        "\n",
        "    # From https://stackoverflow.com/a/42450565/1991373\n",
        "    # Boolean tensor marking where distances are less than relative_disk_radius.\n",
        "    # insides = tf.less(distances, relative_disk_radius)\n",
        "    insides = tf.less(distances, fcd_disk_radius())\n",
        "    map_to_zero_or_one = tf.cast(insides, tf.int32)\n",
        "    return map_to_zero_or_one\n",
        "\n",
        "dependencies = { 'in_disk': in_disk }\n",
        "\n",
        "def read_default_pre_trained_model():\n",
        "    print('Reading pre-trained model from:', trained_model)\n",
        "    return keras.models.load_model(trained_model, custom_objects=dependencies)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FineTuningDataset"
      ],
      "metadata": {
        "id": "xlnzeJgbj6bS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FineTuningDataset:\n",
        "    \"\"\"Manages the dataset of images and labels for fine-tuning.\"\"\"\n",
        "\n",
        "    # Accumulated a new “training set” of the most recent N steps seen so far. (See\n",
        "    # https://cwreynolds.github.io/TexSyn/#20220421 and ...#20220424 for discussion\n",
        "    # of this parameter. Had been 1, then 100, then 200, then finally, infinity.) \n",
        "    # max_training_set_size = float('inf') # keep ALL steps in training set, use GPU.\n",
        "    max_training_set_size = 500 # Try smaller again, \"yellow flowers\" keeps failing.\n",
        "    # List of \"pixel tensors\".\n",
        "    fine_tune_images = []\n",
        "    # List of xy3 [[x,y],[x,y],[x,y]] for 3 prey centers.\n",
        "    fine_tune_labels = []\n",
        "\n",
        "    def update(self, pixel_tensor, prediction, step, directory):\n",
        "        # Assume the predator was \"aiming for\" that one but missed by a bit.\n",
        "        xy3 = read_3_centers_from_file(step, directory)\n",
        "        sorted_xy3 = sort_xy3_by_proximity_to_point(xy3, prediction)\n",
        "\n",
        "        # Accumulate the most recent \"max_training_set_size\" training samples.\n",
        "        self.fine_tune_images.append(pixel_tensor)\n",
        "        self.fine_tune_labels.append(sorted_xy3)\n",
        "\n",
        "        # If training set has become too large, slice off first element of each.\n",
        "        if len(self.fine_tune_images) > self.max_training_set_size:\n",
        "            self.fine_tune_images = self.fine_tune_images[1:]\n",
        "            self.fine_tune_labels = self.fine_tune_labels[1:]\n",
        "\n",
        "        # print('images_array.shape =', images_array.shape,\n",
        "        #       '-- labels_array.shape =', labels_array.shape)\n",
        "        # print('np.shape(self.fine_tune_images) =',\n",
        "        #       np.shape(self.fine_tune_images),\n",
        "        #       '-- np.shape(self.fine_tune_labels) =',\n",
        "        #       np.shape(self.fine_tune_labels))\n",
        "        print('fine_tune_images shape =', np.shape(self.fine_tune_images),\n",
        "              '-- fine_tune_labels shape =', np.shape(self.fine_tune_labels))\n",
        "        \n",
        "\n",
        "# Create a global FineTuningDataset object.\n",
        "# (TODO globals are usually a bad idea, reconsider this.)\n",
        "fine_tuning_dataset = FineTuningDataset()"
      ],
      "metadata": {
        "id": "8Btaj5aXj8iw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predator class"
      ],
      "metadata": {
        "id": "RgRdzOk5Vtmr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Predator:\n",
        "    \"\"\"Represents a Predator in the camouflage simulation. It has a CNN-based\n",
        "       model of visual hunting that identified the position of likely prey.\"\"\"\n",
        "\n",
        "    # Global list of active Predators. (As a class variable.)\n",
        "    population = []\n",
        "\n",
        "    def __init__(self):\n",
        "        # Each Predator has its own copy of a prey-finding CNN-based model.\n",
        "        self.model = []\n",
        "        # By default add this new Predator to the population (TODO reconsider)\n",
        "        Predator.population.append(self)\n",
        "\n",
        "    # Set this Predator's model to default.\n",
        "    def initialize_to_pre_trained_model(self):\n",
        "        self.model = read_default_pre_trained_model()\n",
        "\n",
        "    # This are presumably just for prototyping\n",
        "    def default_predator():\n",
        "        return Predator.population[0]\n",
        "    def second_predator():\n",
        "        return Predator.population[1]\n",
        "    \n",
        "    # Keep track of how often selected prey is nearest center:\n",
        "    nearest_center = 0\n",
        "\n",
        "    # Apply fine-tuning to (originally pre-trained) predator. Use recent steps as\n",
        "    # training set. Assume they were \"near misses\" and so training label is actual\n",
        "    # (ground truth) center of disk nearest prediction. Keep a max number of old\n",
        "    # steps to allow gradually forgetting the earliest part of the run.\n",
        "    def fine_tune_model(self, pixel_tensor, prediction, step, directory):\n",
        "        # Assume the predator was \"aiming for\" that one but missed by a bit.\n",
        "        xy3 = read_3_centers_from_file(step, directory)\n",
        "        sorted_xy3 = sort_xy3_by_proximity_to_point(xy3, prediction)\n",
        "\n",
        "        # keep track of how often selected prey is nearest center:\n",
        "        temp = xy3.copy()  # needed?\n",
        "        sorted_by_dist_to_center = sort_xy3_by_proximity_to_point(temp, [0.5, 0.5])\n",
        "        if sorted_by_dist_to_center[0] == sorted_xy3[0]:\n",
        "            Predator.nearest_center += 1\n",
        "        print('  nearest_center:',\n",
        "              str(int(100 * float(self.nearest_center) / (step + 1))) + '%',\n",
        "              '(nearest_center =', self.nearest_center, ', steps =', step + 1, ')')\n",
        "\n",
        "        # Convert training data list to np arrays\n",
        "        images_array = np.array(fine_tuning_dataset.fine_tune_images)\n",
        "        labels_array = np.array([x[0] for x in fine_tuning_dataset.fine_tune_labels])\n",
        "\n",
        "        # print('images_array.shape =', images_array.shape,\n",
        "        #       '-- labels_array.shape =', labels_array.shape)\n",
        "\n",
        "    \t# Skip fine-tuning until dataset is large enough (10% of max size).\n",
        "        ########################################################################\n",
        "        # TODO 20220803 clone model\n",
        "\n",
        "        # print('disabled \"Skip fine-tuning until dataset is large enough\"')\n",
        "        # # if images_array.shape[0] > (fine_tuning_dataset.max_training_set_size * 0.1):\n",
        "        # if images_array.shape[0] > 0: ##########################################\n",
        "        #     # Do fine-tuning training step using data accumulated during run.\n",
        "        #     history = self.model.fit(x=images_array, y=labels_array)\n",
        "        #     # Keep log of in_disk metric:\n",
        "        #     write_in_disk_log(step, history)\n",
        "\n",
        "        print('disabled \"Skip fine-tuning until dataset is large enough\"')\n",
        "        # if images_array.shape[0] > (fine_tuning_dataset.max_training_set_size * 0.1):\n",
        "        if images_array.shape[0] > 0: ##########################################\n",
        "            # Do fine-tuning training step using data accumulated during run.\n",
        "            history = self.model.fit(x=images_array, y=labels_array)\n",
        "            # Keep log of in_disk metric:\n",
        "            write_in_disk_log(step, history)\n",
        "\n",
        "        ########################################################################\n",
        "    \n",
        "    ############################################################################\n",
        "    # TODO 20220803 clone model\n",
        "    # Copy the neural net model of a given predator into this one.\n",
        "    def copy_model(self, another_predator):\n",
        "        # No this is wrong, just does a shallow copy\n",
        "        # self.model = another_predator.model\n",
        "        self.model = tf.keras.models.clone_model(another_predator.model)\n",
        "        # Compile newly cloned model.\n",
        "        # compile_disk_finder_model(self.model)\n",
        "        df.compile_disk_finder_model(self.model)\n",
        "    ############################################################################\n",
        "\n",
        "    # Modify this Predator's model by adding noise to its weights.\n",
        "    def jiggle_model(self):\n",
        "        weight_perturbation(self.model, 0.001)\n",
        "\n",
        "\n",
        "# Utility based on https://stackoverflow.com/a/64542651/1991373\n",
        "def weight_perturbation(model, max_range):\n",
        "    \"\"\"Add noise to all weights in a Keras model.\"\"\"\n",
        "    for layer in model.layers:\n",
        "        trainable_weights = layer.trainable_variables\n",
        "        for weight in trainable_weights:\n",
        "            random_weights = tf.random.uniform(tf.shape(weight),\n",
        "                                               max_range / -2,\n",
        "                                               max_range / 2,\n",
        "                                               dtype=tf.float32)\n",
        "            weight.assign_add(random_weights)\n",
        "\n",
        "############################################################################\n",
        "# TODO 20220803 clone model\n",
        "#     This should be a utility in DiskFind.py,\n",
        "#     and be called from make_disk_finder_model()\n",
        "\n",
        "# # Compile a disk finder model.\n",
        "# def compile_disk_finder_model(model):\n",
        "#     # Compile with mse loss, tracking accuracy and fraction-inside-disk.\n",
        "#     model.compile(loss='mse', optimizer='adam', metrics=[\"accuracy\", df.in_disk])\n",
        "\n",
        "# # Compile a disk finder model.\n",
        "# def compile_disk_finder_model(model):\n",
        "#     # Compile with mse loss, tracking accuracy and fraction-inside-disk.\n",
        "#     model.compile(loss='mse', optimizer='adam', metrics=[\"accuracy\", in_disk])\n",
        "\n",
        "############################################################################\n",
        "\n",
        "\n",
        "# Global Predator instances for prototyping.\n",
        "test_predator = Predator()\n",
        "test_predator.initialize_to_pre_trained_model()\n",
        "second_predator = Predator()\n",
        "# second_predator.initialize_to_pre_trained_model()\n",
        "second_predator.copy_model(test_predator)\n",
        "second_predator.jiggle_model()"
      ],
      "metadata": {
        "id": "a4sR8eHOV2pF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48b940a8-849e-40ba-9320-06e531a6d3a5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading pre-trained model from: /content/drive/My Drive/PredatorEye/saved_models/20220321_1711_FCD6_rc4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itBD_Ve0lEYB"
      },
      "source": [
        "# Run test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZQLL52Sk-X9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d084ba0-0667-4500-f6bc-ccb8f7609e03"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start run in /content/drive/My Drive/PredatorEye/evo_camo_vs_static_fcd/\n",
            "Waiting for camo_0.png and prey_0.txt ... done, elapsed time: 46 seconds.\n",
            "Wrote '0.55099493 0.7573799' to response file find_0.txt\n",
            "\"other\" prediction: [-0.00034483 -0.00056956]  distance to original prediction: 0.93726355\n",
            "fine_tune_images shape = (1, 128, 128, 3) -- fine_tune_labels shape = (1, 3, 2)\n",
            "  nearest_center: 0% (nearest_center = 0 , steps = 1 )\n",
            "disabled \"Skip fine-tuning until dataset is large enough\"\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0231 - accuracy: 1.0000 - in_disk: 0.0000e+00\n",
            "  nearest_center: 0% (nearest_center = 0 , steps = 1 )\n",
            "disabled \"Skip fine-tuning until dataset is large enough\"\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.5078 - accuracy: 0.0000e+00 - in_disk: 0.0000e+00\n",
            "Waiting for camo_1.png and prey_1.txt ... done, elapsed time: 26 seconds.\n",
            "Wrote '0.60588676 0.85103744' to response file find_1.txt\n",
            "\"other\" prediction: [0.21140143 0.26112258]  distance to original prediction: 0.70966065\n",
            "fine_tune_images shape = (2, 128, 128, 3) -- fine_tune_labels shape = (2, 3, 2)\n",
            "  nearest_center: 0% (nearest_center = 0 , steps = 2 )\n",
            "disabled \"Skip fine-tuning until dataset is large enough\"\n",
            "1/1 [==============================] - 0s 213ms/step - loss: 0.0067 - accuracy: 1.0000 - in_disk: 0.5000\n",
            "  nearest_center: 0% (nearest_center = 0 , steps = 2 )\n",
            "disabled \"Skip fine-tuning until dataset is large enough\"\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.2026 - accuracy: 0.5000 - in_disk: 0.0000e+00\n",
            "Waiting for camo_2.png and prey_2.txt ... done, elapsed time: 39 seconds.\n",
            "Wrote '0.570056 0.5995372' to response file find_2.txt\n",
            "\"other\" prediction: [1.3174529 1.9646947]  distance to original prediction: 1.5563604\n",
            "fine_tune_images shape = (3, 128, 128, 3) -- fine_tune_labels shape = (3, 3, 2)\n",
            "  nearest_center: 33% (nearest_center = 1 , steps = 3 )\n",
            "disabled \"Skip fine-tuning until dataset is large enough\"\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0237 - accuracy: 1.0000 - in_disk: 0.0000e+00\n",
            "  nearest_center: 66% (nearest_center = 2 , steps = 3 )\n",
            "disabled \"Skip fine-tuning until dataset is large enough\"\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.9388 - accuracy: 0.6667 - in_disk: 0.0000e+00\n",
            "Waiting for camo_3.png and prey_3.txt ... done, elapsed time: 39 seconds.\n",
            "Wrote '0.50411266 0.5155136' to response file find_3.txt\n",
            "\"other\" prediction: [0.30924928 0.42828926]  distance to original prediction: 0.21349432\n",
            "fine_tune_images shape = (4, 128, 128, 3) -- fine_tune_labels shape = (4, 3, 2)\n",
            "  nearest_center: 75% (nearest_center = 3 , steps = 4 )\n",
            "disabled \"Skip fine-tuning until dataset is large enough\"\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.0160 - accuracy: 1.0000 - in_disk: 0.0000e+00\n",
            "  nearest_center: 100% (nearest_center = 4 , steps = 4 )\n",
            "disabled \"Skip fine-tuning until dataset is large enough\"\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0380 - accuracy: 1.0000 - in_disk: 0.0000e+00\n",
            "Waiting for camo_4.png and prey_4.txt ... done, elapsed time: 40 seconds.\n",
            "Wrote '0.6834878 0.30001634' to response file find_4.txt\n",
            "\"other\" prediction: [0.11386106 0.14177011]  distance to original prediction: 0.59119916\n",
            "fine_tune_images shape = (5, 128, 128, 3) -- fine_tune_labels shape = (5, 3, 2)\n",
            "  nearest_center: 80% (nearest_center = 4 , steps = 5 )\n",
            "disabled \"Skip fine-tuning until dataset is large enough\"\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.0250 - accuracy: 0.8000 - in_disk: 0.4000\n",
            "  nearest_center: 80% (nearest_center = 4 , steps = 5 )\n",
            "disabled \"Skip fine-tuning until dataset is large enough\"\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.1582 - accuracy: 0.6000 - in_disk: 0.0000e+00\n",
            "Waiting for camo_5.png and prey_5.txt ... done, elapsed time: 40 seconds.\n",
            "Wrote '0.35024405 0.86205363' to response file find_5.txt\n",
            "\"other\" prediction: [0.09403121 0.11716687]  distance to original prediction: 0.7877191\n",
            "fine_tune_images shape = (6, 128, 128, 3) -- fine_tune_labels shape = (6, 3, 2)\n",
            "  nearest_center: 66% (nearest_center = 4 , steps = 6 )\n",
            "disabled \"Skip fine-tuning until dataset is large enough\"\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.0239 - accuracy: 0.8333 - in_disk: 0.3333\n",
            "  nearest_center: 66% (nearest_center = 4 , steps = 6 )\n",
            "disabled \"Skip fine-tuning until dataset is large enough\"\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.3182 - accuracy: 0.6667 - in_disk: 0.0000e+00\n",
            "Waiting for camo_6.png and prey_6.txt ... done, elapsed time: 60 seconds.\n",
            "Wrote '0.4622477 0.3950569' to response file find_6.txt\n",
            "\"other\" prediction: [0.10585772 0.13191457]  distance to original prediction: 0.44300985\n",
            "fine_tune_images shape = (7, 128, 128, 3) -- fine_tune_labels shape = (7, 3, 2)\n",
            "  nearest_center: 71% (nearest_center = 5 , steps = 7 )\n",
            "disabled \"Skip fine-tuning until dataset is large enough\"\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.0076 - accuracy: 1.0000 - in_disk: 0.5714\n",
            "  nearest_center: 85% (nearest_center = 6 , steps = 7 )\n",
            "disabled \"Skip fine-tuning until dataset is large enough\"\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2454 - accuracy: 0.4286 - in_disk: 0.0000e+00\n",
            "Waiting for camo_7.png and prey_7.txt ... done, elapsed time: 19 seconds.\n",
            "Wrote '0.49449426 0.71308696' to response file find_7.txt\n",
            "\"other\" prediction: [0.13820384 0.1721861 ]  distance to original prediction: 0.64770097\n",
            "fine_tune_images shape = (8, 128, 128, 3) -- fine_tune_labels shape = (8, 3, 2)\n",
            "  nearest_center: 87% (nearest_center = 7 , steps = 8 )\n",
            "disabled \"Skip fine-tuning until dataset is large enough\"\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.0122 - accuracy: 1.0000 - in_disk: 0.2500\n",
            "  nearest_center: 100% (nearest_center = 8 , steps = 8 )\n",
            "disabled \"Skip fine-tuning until dataset is large enough\"\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.1624 - accuracy: 0.6250 - in_disk: 0.0000e+00\n",
            "Waiting for camo_8.png and prey_8.txt ... done, elapsed time: 40 seconds.\n",
            "Wrote '0.3072219 0.40541583' to response file find_8.txt\n",
            "\"other\" prediction: [0.2312921  0.28812304]  distance to original prediction: 0.13972448\n",
            "fine_tune_images shape = (9, 128, 128, 3) -- fine_tune_labels shape = (9, 3, 2)\n",
            "  nearest_center: 88% (nearest_center = 8 , steps = 9 )\n",
            "disabled \"Skip fine-tuning until dataset is large enough\"\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.0194 - accuracy: 0.8889 - in_disk: 0.2222\n",
            "  nearest_center: 88% (nearest_center = 8 , steps = 9 )\n",
            "disabled \"Skip fine-tuning until dataset is large enough\"\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.1204 - accuracy: 0.6667 - in_disk: 0.1111\n",
            "Waiting for camo_9.png and prey_9.txt ... done, elapsed time: 19 seconds.\n",
            "Wrote '0.58023846 0.6596591' to response file find_9.txt\n",
            "\"other\" prediction: [0.38456324 0.47896573]  distance to original prediction: 0.26634353\n",
            "fine_tune_images shape = (10, 128, 128, 3) -- fine_tune_labels shape = (10, 3, 2)\n",
            "  nearest_center: 80% (nearest_center = 8 , steps = 10 )\n",
            "disabled \"Skip fine-tuning until dataset is large enough\"\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.0138 - accuracy: 0.9000 - in_disk: 0.3000\n",
            "  nearest_center: 80% (nearest_center = 8 , steps = 10 )\n",
            "disabled \"Skip fine-tuning until dataset is large enough\"\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0702 - accuracy: 0.7000 - in_disk: 0.0000e+00\n",
            "Waiting for camo_10.png and prey_10.txt ... done, elapsed time: 19 seconds.\n",
            "Wrote '0.59531534 0.7780913' to response file find_10.txt\n",
            "\"other\" prediction: [0.6122475  0.76127934]  distance to original prediction: 0.023860872\n",
            "fine_tune_images shape = (11, 128, 128, 3) -- fine_tune_labels shape = (11, 3, 2)\n",
            "  nearest_center: 72% (nearest_center = 8 , steps = 11 )\n",
            "disabled \"Skip fine-tuning until dataset is large enough\"\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.0110 - accuracy: 0.9091 - in_disk: 0.1818\n",
            "  nearest_center: 72% (nearest_center = 8 , steps = 11 )\n",
            "disabled \"Skip fine-tuning until dataset is large enough\"\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.1915 - accuracy: 0.7273 - in_disk: 0.1818\n",
            "Waiting for camo_11.png and prey_11.txt ... done, elapsed time: 19 seconds.\n",
            "Wrote '0.5793744 0.6685345' to response file find_11.txt\n",
            "\"other\" prediction: [0.47632816 0.590961  ]  distance to original prediction: 0.12898129\n",
            "fine_tune_images shape = (12, 128, 128, 3) -- fine_tune_labels shape = (12, 3, 2)\n",
            "  nearest_center: 75% (nearest_center = 9 , steps = 12 )\n",
            "disabled \"Skip fine-tuning until dataset is large enough\"\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 0.0312 - accuracy: 0.7500 - in_disk: 0.3333\n",
            "  nearest_center: 83% (nearest_center = 10 , steps = 12 )\n",
            "disabled \"Skip fine-tuning until dataset is large enough\"\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.1010 - accuracy: 0.7500 - in_disk: 0.0000e+00\n",
            "Waiting for camo_12.png and prey_12.txt ... done, elapsed time: 19 seconds.\n",
            "Wrote '0.5863265 0.7905283' to response file find_12.txt\n",
            "\"other\" prediction: [0.32205966 0.3985126 ]  distance to original prediction: 0.47277188\n",
            "fine_tune_images shape = (13, 128, 128, 3) -- fine_tune_labels shape = (13, 3, 2)\n",
            "  nearest_center: 76% (nearest_center = 10 , steps = 13 )\n",
            "disabled \"Skip fine-tuning until dataset is large enough\"\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 0.0262 - accuracy: 0.7692 - in_disk: 0.2308\n",
            "  nearest_center: 76% (nearest_center = 10 , steps = 13 )\n",
            "disabled \"Skip fine-tuning until dataset is large enough\"\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0489 - accuracy: 0.6923 - in_disk: 0.0769\n",
            "Waiting for camo_13.png and prey_13.txt ... done, elapsed time: 19 seconds.\n",
            "Wrote '0.57765746 0.6753445' to response file find_13.txt\n",
            "\"other\" prediction: [0.28380182 0.35118148]  distance to original prediction: 0.43753037\n",
            "fine_tune_images shape = (14, 128, 128, 3) -- fine_tune_labels shape = (14, 3, 2)\n",
            "  nearest_center: 78% (nearest_center = 11 , steps = 14 )\n",
            "disabled \"Skip fine-tuning until dataset is large enough\"\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.0142 - accuracy: 0.8571 - in_disk: 0.2857\n",
            "  nearest_center: 85% (nearest_center = 12 , steps = 14 )\n",
            "disabled \"Skip fine-tuning until dataset is large enough\"\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0703 - accuracy: 0.7143 - in_disk: 0.1429\n",
            "Waiting for camo_14.png and prey_14.txt ... done, elapsed time: 19 seconds.\n",
            "Wrote '0.3676847 0.39174944' to response file find_14.txt\n",
            "\"other\" prediction: [0.2587855  0.32017377]  distance to original prediction: 0.13031544\n",
            "fine_tune_images shape = (15, 128, 128, 3) -- fine_tune_labels shape = (15, 3, 2)\n",
            "  nearest_center: 80% (nearest_center = 12 , steps = 15 )\n",
            "disabled \"Skip fine-tuning until dataset is large enough\"\n",
            "1/1 [==============================] - 0s 92ms/step - loss: 0.0174 - accuracy: 0.6667 - in_disk: 0.1333\n",
            "  nearest_center: 80% (nearest_center = 12 , steps = 15 )\n",
            "disabled \"Skip fine-tuning until dataset is large enough\"\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0799 - accuracy: 0.7333 - in_disk: 0.0000e+00\n",
            "Waiting for camo_15.png and prey_15.txt ... done, elapsed time: 40 seconds.\n",
            "Wrote '0.64216715 0.32784846' to response file find_15.txt\n",
            "\"other\" prediction: [0.26365048 0.32617003]  distance to original prediction: 0.3785204\n",
            "fine_tune_images shape = (16, 128, 128, 3) -- fine_tune_labels shape = (16, 3, 2)\n",
            "  nearest_center: 81% (nearest_center = 13 , steps = 16 )\n",
            "disabled \"Skip fine-tuning until dataset is large enough\"\n",
            "1/1 [==============================] - 0s 93ms/step - loss: 0.0101 - accuracy: 0.8125 - in_disk: 0.3125\n",
            "  nearest_center: 87% (nearest_center = 14 , steps = 16 )\n",
            "disabled \"Skip fine-tuning until dataset is large enough\"\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.0760 - accuracy: 0.6875 - in_disk: 0.0000e+00\n",
            "Waiting for camo_16.png and prey_16.txt ... done, elapsed time: 19 seconds.\n",
            "Wrote '0.57238233 0.68896127' to response file find_16.txt\n",
            "\"other\" prediction: [0.27730867 0.34310904]  distance to original prediction: 0.45462316\n",
            "fine_tune_images shape = (17, 128, 128, 3) -- fine_tune_labels shape = (17, 3, 2)\n",
            "  nearest_center: 82% (nearest_center = 14 , steps = 17 )\n",
            "disabled \"Skip fine-tuning until dataset is large enough\"\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 0.0253 - accuracy: 0.8235 - in_disk: 0.2941\n",
            "  nearest_center: 82% (nearest_center = 14 , steps = 17 )\n",
            "disabled \"Skip fine-tuning until dataset is large enough\"\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.0782 - accuracy: 0.6471 - in_disk: 0.0588\n",
            "Waiting for camo_17.png and prey_17.txt ... done, elapsed time: 19 seconds.\n",
            "Wrote '0.29978657 0.26226404' to response file find_17.txt\n",
            "\"other\" prediction: [0.30469888 0.37689814]  distance to original prediction: 0.1147393\n",
            "fine_tune_images shape = (18, 128, 128, 3) -- fine_tune_labels shape = (18, 3, 2)\n",
            "  nearest_center: 77% (nearest_center = 14 , steps = 18 )\n",
            "disabled \"Skip fine-tuning until dataset is large enough\"\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.0149 - accuracy: 0.7222 - in_disk: 0.3889\n",
            "  nearest_center: 77% (nearest_center = 14 , steps = 18 )\n",
            "disabled \"Skip fine-tuning until dataset is large enough\"\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0534 - accuracy: 0.6111 - in_disk: 0.0000e+00\n",
            "Waiting for camo_18.png and prey_18.txt ... done, elapsed time: 19 seconds.\n",
            "Wrote '0.6690969 0.668436' to response file find_18.txt\n",
            "\"other\" prediction: [0.3729865 0.4613657]  distance to original prediction: 0.36133015\n",
            "fine_tune_images shape = (19, 128, 128, 3) -- fine_tune_labels shape = (19, 3, 2)\n",
            "  nearest_center: 78% (nearest_center = 15 , steps = 19 )\n",
            "disabled \"Skip fine-tuning until dataset is large enough\"\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 0.0082 - accuracy: 0.8421 - in_disk: 0.4737\n",
            "  nearest_center: 84% (nearest_center = 16 , steps = 19 )\n",
            "disabled \"Skip fine-tuning until dataset is large enough\"\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0549 - accuracy: 0.6316 - in_disk: 0.0000e+00\n",
            "Waiting for camo_19.png and prey_19.txt ... done, elapsed time: 19 seconds.\n",
            "Wrote '0.6375811 0.312644' to response file find_19.txt\n",
            "\"other\" prediction: [0.41331464 0.5111128 ]  distance to original prediction: 0.29947507\n",
            "fine_tune_images shape = (20, 128, 128, 3) -- fine_tune_labels shape = (20, 3, 2)\n",
            "  nearest_center: 85% (nearest_center = 17 , steps = 20 )\n",
            "disabled \"Skip fine-tuning until dataset is large enough\"\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 0.0125 - accuracy: 0.8000 - in_disk: 0.6000\n",
            "  nearest_center: 90% (nearest_center = 18 , steps = 20 )\n",
            "disabled \"Skip fine-tuning until dataset is large enough\"\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.0716 - accuracy: 0.5500 - in_disk: 0.0500\n",
            "Waiting for camo_20.png and prey_20.txt ..."
          ]
        }
      ],
      "source": [
        "# Keep track of how often selected prey is nearest center:\n",
        "Predator.nearest_center = 0\n",
        "\n",
        "# Predator.population = []\n",
        "# TODO maybe a Predator.reset() method?\n",
        "\n",
        "# Flush out obsolete files in comms directory.\n",
        "clean_up_communication_directory()\n",
        "\n",
        "# Start fresh run defaulting to step 0.\n",
        "start_run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7h7-m6IIu6i4"
      },
      "outputs": [],
      "source": [
        "# Normally start from step 0, or if an \"other\" file exists\n",
        "# (eg 'camo_123.jpeg') then restart from that point.\n",
        "\n",
        "# restart_run()"
      ]
    }
  ]
}