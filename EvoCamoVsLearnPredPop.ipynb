{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g8AbtMQfMUkt"
   },
   "source": [
    "# Evolutionary Camouflage Versus a Learning Predator Population\n",
    "\n",
    "---\n",
    "\n",
    "**EvoCamoVsLearnPredPop.ipynb**\n",
    "\n",
    "August 23, 2022: this version runs “local mode” with both predator and prey running on the same machine.\n",
    "\n",
    "(The former mode can be set with `Rube_Goldberg_mode = True`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cr9fUxZxJBRl",
    "outputId": "593bfef6-8130-43c7-8bff-6cba0a4a8ac6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rube_Goldberg_mode = False\n",
      "shared_directory = /Users/cwr/camo_data/comms/\n",
      "saved_model_directory = /Users/cwr/Library/CloudStorage/GoogleDrive-craig.w.reynolds@gmail.com/My Drive/PredatorEye/saved_models/\n",
      "TensorFlow version: 2.9.2\n"
     ]
    }
   ],
   "source": [
    "# \"Rube Goldberg\" mode refers to running camouflage evolution on my laptop while\n",
    "# running predator vision in cloud via Colab. State is passed back and forth via\n",
    "# files on Google Drive.\n",
    "\n",
    "# TODO 20220822\n",
    "# Rube_Goldberg_mode = True\n",
    "Rube_Goldberg_mode = False\n",
    "\n",
    "def if_RG_mode(for_RG_mode, for_normal_mode):\n",
    "    return for_RG_mode if Rube_Goldberg_mode else for_normal_mode\n",
    "\n",
    "# PredatorEye directory on Drive.\n",
    "pe_directory = '/content/drive/My Drive/PredatorEye/'\n",
    "\n",
    "# Shared \"communication\" directory on Drive.\n",
    "# TODO 20220822\n",
    "# shared_directory = if_RG_mode(pe_directory + 'evo_camo_vs_static_fcd/',\n",
    "#                               '/Users/cwr/comms/')\n",
    "shared_directory = if_RG_mode(pe_directory + 'evo_camo_vs_static_fcd/',\n",
    "                              '/Users/cwr/camo_data/comms/')\n",
    "\n",
    "# This was meant (20220716) to allow reading original pre-trained model from\n",
    "# Google Drive, but I'll need to retrain it for M1 (Apple Silicon).\n",
    "g_drive_pe_dir = ('/Users/cwr/Library/CloudStorage/' +\n",
    "                  'GoogleDrive-craig.w.reynolds@gmail.com/' +\n",
    "                  'My Drive/PredatorEye/')\n",
    "\n",
    "# Directory for pre-trained Keras/TensorFlow models.\n",
    "saved_model_directory = if_RG_mode(pe_directory, g_drive_pe_dir) + 'saved_models/'\n",
    "\n",
    "\n",
    "print('Rube_Goldberg_mode =', Rube_Goldberg_mode)\n",
    "print('shared_directory =', shared_directory)\n",
    "print('saved_model_directory =', saved_model_directory)\n",
    "\n",
    "# Pathname of pre-trained Keras/TensorFlow model\n",
    "# trained_model = saved_model_directory + '20220202_1211_Find_3_Disks_complex'\n",
    "# trained_model = saved_model_directory + '20220222_1747_F3D_augmented_rc4'\n",
    "# trained_model = saved_model_directory + '20220227_0746_F3D2_a'\n",
    "# trained_model = saved_model_directory + '20220304_1135_FCD5_a'\n",
    "trained_model = saved_model_directory + '20220321_1711_FCD6_rc4'\n",
    "# model = []\n",
    "\n",
    "# Directory on Drive for storing fine-tuning dataset.\n",
    "fine_tuning_directory = shared_directory + 'fine_tuning/'\n",
    "\n",
    "my_prefix = \"find_\"\n",
    "other_prefix = \"camo_\"\n",
    "\n",
    "my_suffix =  \".txt\"\n",
    "# other_suffix = \".jpeg\"\n",
    "other_suffix = \".png\"\n",
    "\n",
    "fcd_image_size = 1024\n",
    "fcd_disk_size = 201\n",
    "\n",
    "import time\n",
    "import PIL\n",
    "from pathlib import Path\n",
    "\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# %tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "print('TensorFlow version:', tf.__version__)\n",
    "\n",
    "from tensorflow.keras import backend as keras_backend\n",
    "keras_backend.set_image_data_format('channels_last')\n",
    "\n",
    "# Import DiskFind utilities for PredatorEye.\n",
    "import sys\n",
    "if Rube_Goldberg_mode:\n",
    "    sys.path.append('/content/drive/My Drive/PredatorEye/shared_code/')\n",
    "else:\n",
    "    sys.path.append('/Users/cwr/Documents/code/PredatorEye/')\n",
    "import DiskFind as df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jCxVUKpMTEcN"
   },
   "source": [
    "# Ad hoc “predator server”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ZF4XLFSBTKOM"
   },
   "outputs": [],
   "source": [
    "# Top level: wait for camo_xxx.jpeg files to appear, respond with find_xxx.txt\n",
    "def start_run(step = 0):\n",
    "    if step == 0:\n",
    "        print('Start run in', shared_directory )\n",
    "    else:\n",
    "        print('Continue run at step', step, ' in', shared_directory)\n",
    "    while True:\n",
    "        perform_step(step, shared_directory)\n",
    "        step += 1\n",
    "\n",
    "# Continue from from the last camo_xxx.jpeg file.\n",
    "def restart_run():\n",
    "    start_run(newest_file_from_other(shared_directory))\n",
    "\n",
    "# Single step: wait for camo file, write response, delete previous response.\n",
    "def perform_step(step, directory):\n",
    "    wait_for_reply(step, shared_directory)\n",
    "    write_response_file(step, shared_directory)\n",
    "    delete_find_file(step - 1, shared_directory)\n",
    "\n",
    "# Read image file for step, apply pre-trained model, write response file.\n",
    "def write_response_file(step, directory):\n",
    "    # Read image file and check for expected format.\n",
    "    image_pathname = make_camo_pathname(step, directory)\n",
    "    pixel_tensor = df.read_image_file_as_pixel_tensor(image_pathname)\n",
    "    assert df.check_pixel_tensor(pixel_tensor), ('wrong file format: ' +\n",
    "                                                 image_pathname)\n",
    "    # Default Predator's model -- for prototying.\n",
    "    model = Predator.default_predator().model\n",
    "    # Run Predator's model on new image.\n",
    "    prediction = model.predict(tf.convert_to_tensor([pixel_tensor]))[0]\n",
    "    \n",
    "        \n",
    "    ###############################################################################\n",
    "    # TODO 20220822 move this up before response file write\n",
    "    # Merge this step's image into fine-tuning dataset, and related bookkeeping.\n",
    "    fine_tuning_dataset.update(pixel_tensor, prediction, step, directory)\n",
    "    ###############################################################################\n",
    "\n",
    "    \n",
    "    # Generate response file.\n",
    "    response_string = str(prediction[0]) + \" \" + str(prediction[1])\n",
    "    verify_comms_directory_reachable()\n",
    "    with open(make_find_pathname(step, directory), 'w') as file:\n",
    "        file.write(response_string)\n",
    "    print('Wrote ' + \"'\" + response_string + \"'\",\n",
    "          'to response file', Path(make_find_pathname(step, directory)).name)\n",
    "    # Logging, maybe only for testing, delete me?\n",
    "    sp = Predator.second_predator()\n",
    "    spm_predict = sp.model.predict(tf.convert_to_tensor([pixel_tensor]))[0]\n",
    "    print('\"other\" prediction:', spm_predict,\n",
    "          ' distance to original prediction:', df.dist2d(spm_predict, prediction))\n",
    "\n",
    "    ###############################################################################\n",
    "    # TODO 20220822 move this up before response file write\n",
    "    #     # Merge this step's image into fine-tuning dataset, and related bookkeeping.\n",
    "    #     fine_tuning_dataset.update(pixel_tensor, prediction, step, directory)\n",
    "    ###############################################################################\n",
    "\n",
    "\n",
    "    # Predator learns from recent experience.\n",
    "    print('fine-tune default_predator:')\n",
    "    Predator.default_predator().fine_tune_model(pixel_tensor, prediction,\n",
    "                                                step, directory)\n",
    "    # For testing/prototyping\n",
    "    print('fine-tune second_predator:')\n",
    "    sp.fine_tune_model(pixel_tensor, prediction, step, directory)\n",
    "\n",
    "# Delete the given file, usually after having written the next one.\n",
    "def delete_find_file(step, directory):\n",
    "    # Why doesn't pathlib provide a Path.remove() method like os?\n",
    "    # TODO oh, missing_ok was added at pathlib version 3.8.\n",
    "    # Path(makeMyPathname(step, directory)).unlink(missing_ok=True)\n",
    "    p = Path(make_find_pathname(step, directory))\n",
    "    if p.exists():\n",
    "        p.unlink()\n",
    "\n",
    "# Delete any remaining file in commuications directory to start a new run.\n",
    "def clean_up_communication_directory():\n",
    "    def delete_directory_contents(directory_path):\n",
    "        for path in directory_path.iterdir():\n",
    "            print('Removing from communication directory:', path)\n",
    "            if path.is_dir():\n",
    "                delete_directory_contents(path)\n",
    "                path.rmdir()\n",
    "            else:\n",
    "                path.unlink()\n",
    "    delete_directory_contents(Path(shared_directory))\n",
    "\n",
    "# From pathname for file of given step number from the \"other\" agent.\n",
    "def make_camo_pathname(step, directory):\n",
    "    return directory + other_prefix + str(step) + other_suffix\n",
    "\n",
    "# Form pathname for \"find_xx.txt\" response file from \"this\" agent.\n",
    "def make_find_pathname(step, directory):\n",
    "    return directory + my_prefix + str(step) + my_suffix\n",
    "\n",
    "# Form pathname for \"prey_xx.txt\" ground truth file from \"other\" agent.\n",
    "def make_prey_pathname(step, directory):\n",
    "    return directory + 'prey_' + str(step) + '.txt'\n",
    "\n",
    "# Used to ping the comms directory when it seems hung.\n",
    "def write_ping_file(count, step, directory):\n",
    "    pn = directory + 'ping_cloud_' + str(step) + '.txt'\n",
    "    verify_comms_directory_reachable()\n",
    "    with open(pn, 'w') as file:\n",
    "        file.write(str(count))\n",
    "    print('Ping comms: ', count, pn)\n",
    "\n",
    "# Wait until other agent's file for given step appears.\n",
    "def wait_for_reply(step, directory):\n",
    "    camo_pathname = Path(make_camo_pathname(step, directory))\n",
    "    camo_filename = camo_pathname.name\n",
    "    prey_pathname = Path(make_prey_pathname(step, directory))\n",
    "    prey_filename = prey_pathname.name\n",
    "    print('Waiting for', camo_filename, 'and', prey_filename, '...',\n",
    "          end='', flush=True)\n",
    "    start_time = time.time()\n",
    "    # Loop until both files are present, waiting 1 second between tests.\n",
    "    test_count = 0\n",
    "    while not (is_file_present(camo_pathname) and\n",
    "               is_file_present(prey_pathname)):\n",
    "        time.sleep(1)\n",
    "        test_count += 1\n",
    "        if (test_count % 100) == 0:\n",
    "            write_ping_file(test_count, step, directory)\n",
    "    print(' done, elapsed time:', int(time.time() - start_time), 'seconds.')\n",
    "\n",
    "# Like fs::exists()\n",
    "def is_file_present(file):\n",
    "    result = False\n",
    "    verify_comms_directory_reachable()\n",
    "    filename = Path(file).name\n",
    "    directory = Path(file).parent\n",
    "    for i in directory.iterdir():\n",
    "        if i.name == filename:\n",
    "            result = True\n",
    "    return result\n",
    "\n",
    "# Returns the step number of the newest file from \"other\" in given directory.\n",
    "# (So if \"camo_573.jpeg\" is the only \"other\" file there, returns int 573)\n",
    "def newest_file_from_other(directory):\n",
    "    steps = [0]  # Default to zero in case dir is empty.\n",
    "    for filename in Path(directory).iterdir():\n",
    "        name = filename.name\n",
    "        if other_prefix == name[0:len(other_prefix)]:\n",
    "            steps.append(int(name.split(\".\")[0].split(\"_\")[1]))\n",
    "    return max(steps)\n",
    "\n",
    "# Read ground truth prey center location data provided in \"prey_n.txt\" file.\n",
    "def read_3_centers_from_file(step, directory):\n",
    "    # Read contents of file as string.\n",
    "    verify_comms_directory_reachable()\n",
    "    with open(make_prey_pathname(step, directory), 'r') as file:\n",
    "        prey_centers_string = file.read()\n",
    "    # Split string at whitespace, map to 6 floats, reshape into 3 xy pairs.\n",
    "    # (TODO could probably be rewritten cleaner with \"list comprehension\")\n",
    "    array = np.reshape(list(map(float, prey_centers_string.split())), (3, 2))\n",
    "    return array.tolist()\n",
    "\n",
    "# Keep log of in_disk metric.\n",
    "def write_in_disk_log(step, history):\n",
    "    if step % 10 == 0:\n",
    "        in_disk = history.history[\"in_disk\"][0]\n",
    "        pathname = shared_directory + 'in_disk_log.csv'\n",
    "        verify_comms_directory_reachable()\n",
    "        with open(pathname, 'a') as file:\n",
    "            if step == 0:\n",
    "                file.write('step,in_disk\\n')\n",
    "            file.write(str(step) + ',' + \"{:.4f}\".format(in_disk) + '\\n')\n",
    "\n",
    "# Just wait in retry loop if shared \"comms\" directory become unreachable.\n",
    "# Probably will return shortly, better to wait than signal a file error.\n",
    "# (This is called from places with a local \"directory\" but it uses global value.)\n",
    "def verify_comms_directory_reachable():\n",
    "    seconds = 0\n",
    "    # shared_directory_pathname = Path(shared_directory)\n",
    "    # while not shared_directory_pathname.is_dir():\n",
    "    while not Path(shared_directory).is_dir():\n",
    "        print(\"Shared “comms” directory,\", shared_directory, \n",
    "              \"has been inaccessible for\", seconds, \"seconds.\")\n",
    "        time.sleep(1)  # wait 1 sec\n",
    "        seconds += 1\n",
    "\n",
    "# Given 3 prey positions (\"xy3\"), sort them by proximity to \"point\" (prediction)\n",
    "def sort_xy3_by_proximity_to_point(xy3, point):\n",
    "    # print('xy3 =', xy3)\n",
    "    xy3_plus_distance = [[df.dist2d(xy, point), xy] for xy in xy3]\n",
    "    # print('xy3_plus_distance =', xy3_plus_distance)\n",
    "    sorted_xy3_plus_key = sorted(xy3_plus_distance, key=lambda x: x[0])\n",
    "    # print('sorted_xy3_plus_key =', sorted_xy3_plus_key)\n",
    "    sorted_xy3 = [x[1] for x in sorted_xy3_plus_key]\n",
    "    # print('sorted_xy3 =', sorted_xy3)\n",
    "    return sorted_xy3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VDtbk88sVGxk"
   },
   "source": [
    "# Read pre-trained model\n",
    "\n",
    "As I integrate this into the Predator class, this is no longer “Read pre-trained model” but more like “Some utilities for reading the pre-trained model”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "iLVIRi_vU9gN"
   },
   "outputs": [],
   "source": [
    "# Read pre-trained TensorFlow \"predator vision\" model.\n",
    "\n",
    "# print('Reading pre-trained model from:', trained_model)\n",
    "\n",
    "# ad hoc workaround suggested on https://stackoverflow.com/q/66408995/1991373\n",
    "#\n",
    "# dependencies = {\n",
    "#     'hamming_loss': tfa.metrics.HammingLoss(mode=\"multilabel\", name=\"hamming_loss\"),\n",
    "#     'attention': attention(return_sequences=True)\n",
    "# }\n",
    "#\n",
    "# dependencies = {\n",
    "#     'valid_accuracy': ValidAccuracy\n",
    "# }\n",
    "\n",
    "# Calculates RELATIVE disk radius on the fly -- rewrite later.\n",
    "def fcd_disk_radius():\n",
    "    return (float(fcd_disk_size) / float(fcd_image_size)) / 2\n",
    "\n",
    "# Given two tensors of 2d point coordinates, return a tensor of the Cartesian\n",
    "# distance between corresponding points in the input tensors.\n",
    "def corresponding_distances(y_true, y_pred):\n",
    "    true_pos_x, true_pos_y = tf.split(y_true, num_or_size_splits=2, axis=1)\n",
    "    pred_pos_x, pred_pos_y = tf.split(y_pred, num_or_size_splits=2, axis=1)\n",
    "    dx = true_pos_x - pred_pos_x\n",
    "    dy = true_pos_y - pred_pos_y\n",
    "    distances = tf.sqrt(tf.square(dx) + tf.square(dy))\n",
    "    return distances\n",
    "\n",
    "# 20211231 copied from Find_Concpocuous_Disk\n",
    "def in_disk(y_true, y_pred):\n",
    "    distances = corresponding_distances(y_true, y_pred)\n",
    "    # relative_disk_radius = (float(fcd_disk_size) / float(fcd_image_size)) / 2\n",
    "\n",
    "    # From https://stackoverflow.com/a/42450565/1991373\n",
    "    # Boolean tensor marking where distances are less than relative_disk_radius.\n",
    "    # insides = tf.less(distances, relative_disk_radius)\n",
    "    insides = tf.less(distances, fcd_disk_radius())\n",
    "    map_to_zero_or_one = tf.cast(insides, tf.int32)\n",
    "    return map_to_zero_or_one\n",
    "\n",
    "dependencies = { 'in_disk': in_disk }\n",
    "\n",
    "def read_default_pre_trained_model():\n",
    "    print('Reading pre-trained model from:', trained_model)\n",
    "    return keras.models.load_model(trained_model, custom_objects=dependencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xlnzeJgbj6bS"
   },
   "source": [
    "# FineTuningDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "8Btaj5aXj8iw"
   },
   "outputs": [],
   "source": [
    "class FineTuningDataset:\n",
    "    \"\"\"Manages the dataset of images and labels for fine-tuning.\"\"\"\n",
    "\n",
    "    # Accumulated a new “training set” of the most recent N steps seen so far. (See\n",
    "    # https://cwreynolds.github.io/TexSyn/#20220421 and ...#20220424 for discussion\n",
    "    # of this parameter. Had been 1, then 100, then 200, then finally, infinity.) \n",
    "    # max_training_set_size = float('inf') # keep ALL steps in training set, use GPU.\n",
    "    max_training_set_size = 500 # Try smaller again, \"yellow flowers\" keeps failing.\n",
    "    # List of \"pixel tensors\".\n",
    "    fine_tune_images = []\n",
    "    # List of xy3 [[x,y],[x,y],[x,y]] for 3 prey centers.\n",
    "    fine_tune_labels = []\n",
    "\n",
    "    def update(self, pixel_tensor, prediction, step, directory):\n",
    "        \n",
    "        # TODO 20220822 debugging\n",
    "        time.sleep(5)\n",
    "        \n",
    "        # Assume the predator was \"aiming for\" that one but missed by a bit.\n",
    "        xy3 = read_3_centers_from_file(step, directory)\n",
    "        sorted_xy3 = sort_xy3_by_proximity_to_point(xy3, prediction)\n",
    "\n",
    "        # Accumulate the most recent \"max_training_set_size\" training samples.\n",
    "        self.fine_tune_images.append(pixel_tensor)\n",
    "        self.fine_tune_labels.append(sorted_xy3)\n",
    "\n",
    "        # If training set has become too large, slice off first element of each.\n",
    "        if len(self.fine_tune_images) > self.max_training_set_size:\n",
    "            self.fine_tune_images = self.fine_tune_images[1:]\n",
    "            self.fine_tune_labels = self.fine_tune_labels[1:]\n",
    "\n",
    "        # print('images_array.shape =', images_array.shape,\n",
    "        #       '-- labels_array.shape =', labels_array.shape)\n",
    "        # print('np.shape(self.fine_tune_images) =',\n",
    "        #       np.shape(self.fine_tune_images),\n",
    "        #       '-- np.shape(self.fine_tune_labels) =',\n",
    "        #       np.shape(self.fine_tune_labels))\n",
    "        print('fine_tune_images shape =', np.shape(self.fine_tune_images),\n",
    "              '-- fine_tune_labels shape =', np.shape(self.fine_tune_labels))\n",
    "        \n",
    "\n",
    "# Create a global FineTuningDataset object.\n",
    "# (TODO globals are usually a bad idea, reconsider this.)\n",
    "fine_tuning_dataset = FineTuningDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RgRdzOk5Vtmr"
   },
   "source": [
    "# Predator class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a4sR8eHOV2pF",
    "outputId": "293b0fdd-0888-4840-de81-b779a4cfb754"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading pre-trained model from: /Users/cwr/Library/CloudStorage/GoogleDrive-craig.w.reynolds@gmail.com/My Drive/PredatorEye/saved_models/20220321_1711_FCD6_rc4\n"
     ]
    }
   ],
   "source": [
    "class Predator:\n",
    "    \"\"\"Represents a Predator in the camouflage simulation. It has a CNN-based\n",
    "       model of visual hunting that identified the position of likely prey.\"\"\"\n",
    "\n",
    "    # Global list of active Predators. (As a class variable.)\n",
    "    population = []\n",
    "\n",
    "    def __init__(self):\n",
    "        # Each Predator has its own copy of a prey-finding CNN-based model.\n",
    "        self.model = []\n",
    "        # By default add this new Predator to the population (TODO reconsider)\n",
    "        Predator.population.append(self)\n",
    "\n",
    "    # Set this Predator's model to default.\n",
    "    def initialize_to_pre_trained_model(self):\n",
    "        self.model = read_default_pre_trained_model()\n",
    "\n",
    "    # This are presumably just for prototyping\n",
    "    def default_predator():\n",
    "        return Predator.population[0]\n",
    "    def second_predator():\n",
    "        return Predator.population[1]\n",
    "    \n",
    "    # Keep track of how often selected prey is nearest center:\n",
    "    nearest_center = 0\n",
    "\n",
    "    # Apply fine-tuning to (originally pre-trained) predator. Use recent steps as\n",
    "    # training set. Assume they were \"near misses\" and so training label is actual\n",
    "    # (ground truth) center of disk nearest prediction. Keep a max number of old\n",
    "    # steps to allow gradually forgetting the earliest part of the run.\n",
    "    def fine_tune_model(self, pixel_tensor, prediction, step, directory):\n",
    "        # Assume the predator was \"aiming for\" that one but missed by a bit.\n",
    "        xy3 = read_3_centers_from_file(step, directory)\n",
    "        sorted_xy3 = sort_xy3_by_proximity_to_point(xy3, prediction)\n",
    "\n",
    "        # keep track of how often selected prey is nearest center:\n",
    "        temp = xy3.copy()  # needed?\n",
    "        sorted_by_dist_to_center = sort_xy3_by_proximity_to_point(temp, [0.5, 0.5])\n",
    "        if sorted_by_dist_to_center[0] == sorted_xy3[0]:\n",
    "            Predator.nearest_center += 1\n",
    "        print('  nearest_center:',\n",
    "              str(int(100 * float(self.nearest_center) / (step + 1))) + '%',\n",
    "              '(nearest_center =', self.nearest_center, ', steps =', step + 1, ')')\n",
    "\n",
    "        # Convert training data list to np arrays\n",
    "        images_array = np.array(fine_tuning_dataset.fine_tune_images)\n",
    "        labels_array = np.array([x[0] for x in fine_tuning_dataset.fine_tune_labels])\n",
    "\n",
    "        # print('images_array.shape =', images_array.shape,\n",
    "        #       '-- labels_array.shape =', labels_array.shape)\n",
    "\n",
    "    \t# Skip fine-tuning until dataset is large enough (10% of max size).\n",
    "        ########################################################################\n",
    "        # TODO 20220803 clone model\n",
    "\n",
    "        # print('disabled \"Skip fine-tuning until dataset is large enough\"')\n",
    "        # # if images_array.shape[0] > (fine_tuning_dataset.max_training_set_size * 0.1):\n",
    "        # if images_array.shape[0] > 0: ##########################################\n",
    "        #     # Do fine-tuning training step using data accumulated during run.\n",
    "        #     history = self.model.fit(x=images_array, y=labels_array)\n",
    "        #     # Keep log of in_disk metric:\n",
    "        #     write_in_disk_log(step, history)\n",
    "\n",
    "        # print('disabled \"Skip fine-tuning until dataset is large enough\"')\n",
    "        # # if images_array.shape[0] > (fine_tuning_dataset.max_training_set_size * 0.1):\n",
    "        # if images_array.shape[0] > 0: ##########################################\n",
    "        #     # Do fine-tuning training step using data accumulated during run.\n",
    "        #     history = self.model.fit(x=images_array, y=labels_array)\n",
    "        #     # Keep log of in_disk metric:\n",
    "        #     write_in_disk_log(step, history)\n",
    "\n",
    "        # TODO 20220823 restore \"Skip fine-tuning until dataset is large enough\"\n",
    "        if images_array.shape[0] > (fine_tuning_dataset.max_training_set_size * 0.1):\n",
    "            # # Do fine-tuning training step using data accumulated during run.\n",
    "            # history = self.model.fit(x=images_array, y=labels_array)\n",
    "            \n",
    "            # TODO 20220823 -- run fine-tuning on CPU only.\n",
    "            print('Running on CPU ONLY!')\n",
    "            with tf.device('/cpu:0'):\n",
    "                # Do fine-tuning training step using data accumulated during run.\n",
    "                history = self.model.fit(x=images_array, y=labels_array)\n",
    "            \n",
    "            # Keep log of in_disk metric:\n",
    "            write_in_disk_log(step, history)\n",
    "\n",
    "\n",
    "        ########################################################################\n",
    "    \n",
    "    ############################################################################\n",
    "    # TODO 20220803 clone model\n",
    "\n",
    "    # # Copy the neural net model of a given predator into this one.\n",
    "    # def copy_model(self, another_predator):\n",
    "    #     # No this is wrong, just does a shallow copy\n",
    "    #     # self.model = another_predator.model\n",
    "    #     self.model = tf.keras.models.clone_model(another_predator.model)\n",
    "    #     # Compile newly cloned model.\n",
    "    #     # compile_disk_finder_model(self.model)\n",
    "    #     df.compile_disk_finder_model(self.model)\n",
    "\n",
    "    # Copy the neural net model of a given predator into this one. (From \"Make\n",
    "    # deep copy of keras model\" https://stackoverflow.com/a/54368176/1991373)\n",
    "    def copy_model(self, another_predator):\n",
    "        other_model = another_predator.model\n",
    "        # Clone layer structure of other model.\n",
    "        # self.model = tf.keras.models.clone_model(another_predator.model)\n",
    "        self.model = tf.keras.models.clone_model(other_model)\n",
    "        # Compile newly cloned model.\n",
    "        df.compile_disk_finder_model(self.model)\n",
    "\n",
    "        # Copy weights of other model.\n",
    "        self.model.set_weights(other_model.get_weights())\n",
    "\n",
    "    ############################################################################\n",
    "\n",
    "    # Modify this Predator's model by adding noise to its weights.\n",
    "    def jiggle_model(self):\n",
    "        weight_perturbation(self.model, 0.001)\n",
    "    \n",
    "    \n",
    "    ########################################################################\n",
    "    # TODO 20220826 print a \"diagonal trace\" of this Predator's model\n",
    "    \n",
    "    # TODO maybe print out one value for each layer of the model,\n",
    "    # on the n level take the [n, n] parameter\n",
    "    \n",
    "    def print_model_trace(self):\n",
    "        for layer in self.model.layers:\n",
    "            trainable_weights = layer.trainable_variables\n",
    "            for weight in trainable_weights:\n",
    "                # random_weights = tf.random.uniform(tf.shape(weight),\n",
    "                #                                    max_range / -2,\n",
    "                #                                    max_range / 2,\n",
    "                #                                    dtype=tf.float32)\n",
    "                # weight.assign_add(random_weights)\n",
    "\n",
    "#                 print(1.23, end = \" \")\n",
    "                \n",
    "#                 print(1.23)\n",
    "#                 print(weight)\n",
    "#                 print(tf.shape(weight))\n",
    "                print(tf.shape(weight).numpy().tolist())\n",
    "    \n",
    "                # TODO 20220827 so maybe do the product reduce of that list,\n",
    "                #      reshape as 1d of that size, take zeroth element?\n",
    "\n",
    "\n",
    "                \n",
    "        print()\n",
    "    ########################################################################\n",
    "\n",
    "\n",
    "\n",
    "# Utility based on https://stackoverflow.com/a/64542651/1991373\n",
    "def weight_perturbation(model, max_range):\n",
    "    \"\"\"Add noise to all weights in a Keras model.\"\"\"\n",
    "    for layer in model.layers:\n",
    "        trainable_weights = layer.trainable_variables\n",
    "        for weight in trainable_weights:\n",
    "            random_weights = tf.random.uniform(tf.shape(weight),\n",
    "                                               max_range / -2,\n",
    "                                               max_range / 2,\n",
    "                                               dtype=tf.float32)\n",
    "            weight.assign_add(random_weights)\n",
    "\n",
    "############################################################################\n",
    "# TODO 20220803 clone model\n",
    "#     This should be a utility in DiskFind.py,\n",
    "#     and be called from make_disk_finder_model()\n",
    "\n",
    "# # Compile a disk finder model.\n",
    "# def compile_disk_finder_model(model):\n",
    "#     # Compile with mse loss, tracking accuracy and fraction-inside-disk.\n",
    "#     model.compile(loss='mse', optimizer='adam', metrics=[\"accuracy\", df.in_disk])\n",
    "\n",
    "# # Compile a disk finder model.\n",
    "# def compile_disk_finder_model(model):\n",
    "#     # Compile with mse loss, tracking accuracy and fraction-inside-disk.\n",
    "#     model.compile(loss='mse', optimizer='adam', metrics=[\"accuracy\", in_disk])\n",
    "\n",
    "############################################################################\n",
    "\n",
    "\n",
    "# # Global Predator instances for prototyping.\n",
    "# test_predator = Predator()\n",
    "# test_predator.initialize_to_pre_trained_model()\n",
    "# second_predator = Predator()\n",
    "# # second_predator.initialize_to_pre_trained_model()\n",
    "# second_predator.copy_model(test_predator)\n",
    "# second_predator.jiggle_model()\n",
    "\n",
    "# Global Predator instances for prototyping.\n",
    "test_predator = Predator()\n",
    "test_predator.initialize_to_pre_trained_model()\n",
    "second_predator = Predator()\n",
    "second_predator.copy_model(test_predator)\n",
    "# TODO leave second_predator an exact copy of first_predator to test equivalence\n",
    "#second_predator.jiggle_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "itBD_Ve0lEYB"
   },
   "source": [
    "# Run test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iZQLL52Sk-X9",
    "outputId": "d8cd0a60-3fc8-484e-d32b-2c9ae0b157d2",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 5, 3, 16]\n",
      "[16]\n",
      "[5, 5, 16, 32]\n",
      "[32]\n",
      "[5, 5, 32, 64]\n",
      "[64]\n",
      "[5, 5, 64, 128]\n",
      "[128]\n",
      "[5, 5, 128, 256]\n",
      "[256]\n",
      "[16384, 128]\n",
      "[128]\n",
      "[128, 32]\n",
      "[32]\n",
      "[32, 8]\n",
      "[8]\n",
      "[8, 2]\n",
      "[2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO 20220827 testing print_model_trace\n",
    "\n",
    "test_predator.print_model_trace()\n",
    "\n",
    "\n",
    "\n",
    "# # Keep track of how often selected prey is nearest center:\n",
    "# Predator.nearest_center = 0\n",
    "\n",
    "# # Predator.population = []\n",
    "# # TODO maybe a Predator.reset() method?\n",
    "\n",
    "# # Flush out obsolete files in comms directory.\n",
    "# clean_up_communication_directory()\n",
    "\n",
    "# # Start fresh run defaulting to step 0.\n",
    "# start_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "7h7-m6IIu6i4"
   },
   "outputs": [],
   "source": [
    "# Normally start from step 0, or if an \"other\" file exists\n",
    "# (eg 'camo_123.jpeg') then restart from that point.\n",
    "\n",
    "# restart_run()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "EvoCamoVsLearnPredPop.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
